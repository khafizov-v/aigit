[
    {
        "name": "spec",
        "url": "https://github.com/QuantumFusion-network/spec",
        "issues": [
            {
                "number": 334,
                "title": "Web SDK",
                "state": "open",
                "author": "AlexLgn",
                "labels": [
                    "Drafted"
                ],
                "created_at": "2025-05-15T10:18:17Z",
                "updated_at": "2025-05-21T08:17:06Z",
                "closed_at": null,
                "body": "This case focuses on developing a modern, developer-friendly Web SDK for the Quantum Fusion blockchain. The SDK will serve as the primary interface for web developers to interact with the QF blockchain, enabling seamless integration with decentralized applications. Based on extensive research into developer needs and pain points in the blockchain space, this SDK will prioritize consistent API design, reliable performance, and efficient abstraction of blockchain complexities to deliver an outstanding developer experience\n\n## Objectives\n\n- Create a TypeScript-first Web SDK that connects to the QF blockchain via WebRTC-based protocols, with proper type definitions and intelligent IDE support\n- Implement comprehensive API methods covering core blockchain functionality:\n  - Account and wallet management (create, import, sign)\n  - Transaction creation, signing, and broadcasting\n  - Block and transaction querying with efficient pagination\n  - Smart contract deployment and interaction\n  - (optional) Light client connectivity and state synchronization\n- Implement robust integration with at least three popular wallet extensions (at minimum: MetaMask, Polkadot.js, and one additional extension like Talisman or SubWallet) with a consistent interface that abstracts differences between implementations\n- Design API patterns that follow established conventions from popular blockchain SDKs (ethers.js, web3.js) to reduce learning curve for developers\n- Abstract low-level complexities like SCALE codec encoding/decoding and big number handling to provide a developer-friendly experience\n- Implement resilient connection handling with automatic retry mechanisms and detailed error reporting with severity levels (e.g. `code`, `severity`, `remedy`)\n- Create helper functions for common tasks that follow familiar Web2 patterns, enabling copy-paste implementation of standard operations and that collapse multi-step flows into one call (e.g. `transferToken()`)\n- Design the SDK using composition principles to ensure compatibility with other ecosystem tools and support dApp interoperability\n- Optimize bundle size through tree-shaking and code-splitting, targeting a maximum of 500KB minified and gzipped. \n- Implement a transaction lifecycle tracking system that emits `submitted`, `inBlock`, `finalized`, `error` events\n- Develop a \"Session\" concept that enables confirmation-less recurring transactions with appropriate security constraints\n- Implement S3-compatible API for blockchain storage that covers core storage operations\n- Build a first-party history querying interface with flexible filtering\n- Provide direct access to underlying RPC methods for advanced developers\n- Develop efficient batch transaction processing system for high-volume operations (100,000+)\n- Develop robust tools for token-specific decimal formatting and calculations\n- Create interfaces for periodic and streaming payments\n- Include `simulate(tx)` that returns predicted weight + execution result without chain commit\n- Design for cross-platform compatibility including mobile environments\n- Publish the SDK to npm under `@qf/web-sdk` using semver and automated `npm publish` on `v*` tags\n- Integrate JSDoc/TSDoc comments with the QF Developer Portal so every `npm publish` regenerates API pages automatically\n- (optional) Develop a specialized testing framework tailored for blockchain applications with simulation capabilities, state assertions, and time manipulation\n\n## Acceptance Criteria\n\n- [ ] The SDK successfully connects to the QF testnet and performs basic operations with latency under 200ms in 95% of operations, measured in stable versions of Chrome, Firefox, Safari, and Edge available at the start of development on both desktop and mobile platforms across all supported environments using 10+ Mbps download, 1+ Mbps upload, <100ms ping internet connections\n- [ ] The SDK implements WebRTC-based protocol for direct browser-to-node connectivity, including handling NAT traversal, connection management, and security constraints. Browser console shows `WebRTC:connected` within 5 s on QF testnet from a public network.\n- [ ] All API methods have consistent naming patterns, detailed TypeScript type definitions, and return informative error messages with severity levels that include potential solutions, verified through code review\n- [ ] The SDK successfully connects to at least three popular wallet extensions (at minimum: MetaMask, Polkadot.js, and one additional extension like Talisman or SubWallet) and manages account access permissions with appropriate connection lifecycle management\n- [ ] Wallet connection state persists appropriately between page reloads with user consent, and the SDK handles wallet switching, account changing, and network changes with appropriate event notifications. After page reload the previously-selected account is re-detected if the wallet extension exposes `autoConnect`, otherwise the SDK prompts user — verified by demo page.\n- [ ] The SDK implements a provider/signer separation pattern (e.g. similar to ethers.js), with clear distinction between read-only and transaction signing operations\n- [ ] Transaction flows follow familiar promise-based patterns (`method().send().wait()`) with appropriate event hooks, while BigNumber handling adopts established patterns from widely-used libraries\n- [ ] The SDK successfully abstracts complex operations (SCALE encoding, big number handling, token decimals) while maintaining type safety, verified through test cases with different token types\n- [ ] Connection resilience features automatically handle common network issues (temporary disconnections, timeouts) with exponential backoff, verified through simulated adverse network conditions\n- [ ] The SDK provides helper methods for at least 10 common blockchain operations that would otherwise require multiple API calls, demonstrated through before/after code examples\n- [ ] The SDK's bundle size does not exceed 500KB when minified and gzipped, measured using standard web bundling tools\n- [ ] Transaction lifecycle tracking successfully captures all state changes from submission to finalization, with filterable events and error reporting\n- [ ] The \"Session\" concept enables users to perform repeated operations without requiring signatures for each transaction while maintaining appropriate security boundaries\n- [ ] The S3-compatible storage interface implements core operations (PUT, GET, DELETE, LIST) with appropriate blockchain adaptations\n- [ ] The interface successfully supports single object operations, metadata handling, and bucket-like organization\n- [ ] Storage operations work with both on-chain and decentralized storage options\n- [ ] Integration tests verify functionality across at least 10 common S3 usage patterns\n- [ ] (optional) The SDK successfully processes transaction batches of at least 100,000 operations through automatic chunking\n- [ ] (optional) Performance scaling remains near-linear up to the 100,000 operation threshold\n- [ ] Batch operations provide consistent progress tracking and reporting\n- [ ] The system implements backpressure handling to prevent client resource exhaustion\n- [ ] Error recovery allows continuing batch processing after resolving transient failures\n- [ ] Developers can submit large batches as a logical unit while the SDK handles optimal splitting\n- [ ] Token calculations handle denominations and decimal places correctly across multiple token standards, specifically ERC-20/PSP-22 compatible tokens, QF native tokens, and NFT tokens (ERC-721/PSP-34 compatible), with appropriate unit conversion and formatting utilities verified through comprehensive test cases\n- [ ] Simulation accurately predicts transaction outcomes without submitting to the blockchain\n- [ ] Method chaining is supported for operation configuration where appropriate, with parameter normalization for different input formats\n- [ ] The SDK implements reactive data streams (compatible with RxJS observables) for subscription to blockchain events and state changes\n- [ ] The SDK package is successfully published on npm and can be installed and used with less than 5 lines of setup code\n- [ ] JSDoc comments cover at least 90% of public APIs, including parameters, return values, and examples\n- [ ] TypeScript definition files are correctly exported and compatible with documentation generation tools\n- [ ] Code examples are properly formatted and tested for accuracy\n- [ ] Documentation structure follows the QF Developer Portal's standards\n- [ ] Documentation integration plan is established, including automated notification workflows, review processes, and content synchronization procedures between SDK and Developer Portal repositories\n- [ ] (optional) Internal benchmarks show the QF Web SDK requires fewer lines of code for common operations compared to at least three competitor SDKs, demonstrated through comparison examples\n- [ ] (optional) The testing framework successfully simulates the complete transaction lifecycle without requiring a live blockchain connection, with utilities for state assertion, event verification, and time manipulation\n\n## Related Work\n\n- [Polkadot.js API](https://polkadot.js.org/docs/api/) - Comprehensive reactive API for Substrate-based chains\n- [Ethers.js](https://docs.ethers.org/) - Sets the standard for TypeScript support and API design\n- [Solana Web3.js](https://solana-foundation.github.io/solana-web3.js/) - High-throughput RPC, accounts & transaction helpers for Solana\n- [NEAR API JS](https://near.github.io/near-api-js) - High-level helper for accounts, keys & contract calls\n- [Anchor Framework](https://www.anchor-lang.com/) - Full-stack toolkit for building and testing Solana programs\n- [Alchemy SDK](https://docs.alchemy.com/reference/alchemy-sdk-quickstart) – Robust, strongly-typed RPC wrapper with enhanced error handling\n- [Thirdweb SDK](https://portal.thirdweb.com/typescript) - High-level abstractions over EVM chains and storage\n- [Lens Protocol SDK](https://lens.xyz/docs/protocol/getting-started/typescript) - Well-architected social-graph SDK showing complex domain modelling\n- [IPFS JS Client](https://docs.ipfs.tech/reference/js/api/) – Browser-ready client for decentralized storage interactions\n- [Stripe.js](https://docs.stripe.com/js) - Excellent Web2 payment SDK design to learn from\n- [Reown AppKit (formerly Web3Modal)](https://docs.reown.com/) – Full-stack wallet-connect kit: modal for 100+ wallets plus optional embedded wallets, on-ramp, and swap modules. Illustrates pluggable UX patterns for multi-wallet onboarding\n- [Hardhat](https://hardhat.org/) - Ethereum development environment with advanced testing\n- [Hardhat plugin for Polkadot](https://papermoonio.github.io/polkadot-mkdocs/develop/smart-contracts/dev-environments/hardhat/#hardhat) - Plugin for using Hardhat with Polkadot\n- [Foundry](https://book.getfoundry.sh/) - Testing framework for Solidity development\n- [ink! E2E](https://use.ink/docs/v5/basics/contract-testing/end-to-end-e2e-testing/) - End-to-end testing for ink! smart contracts\n- [Viem](https://viem.sh/) – Modern, lightweight, fully-typed alternative to ethers.js with tiny bundles\n- [Wagmi](https://wagmi.sh/) – React hooks that exemplify ergonomic wallet & contract UX patterns\n\n## Constraints\n\n- Must implement QF's peer-to-peer WebRTC protocol for direct browser-to-node connectivity, including handling NAT traversal, connection management, and security constraints\n- Must maintain compatibility with all major browsers (Chrome, Firefox, Safari, Edge) and their common mobile versions\n- Must handle Substrate's SCALE encoding/decoding for blockchain data format\n- Must support QF smart contracts via PolkaVM with appropriate abstraction\n- Bundle size must not exceed 500KB minified and gzipped to ensure reasonable page load performance\n- Must follow the principle of progressive disclosure, allowing simple usage for beginners while enabling advanced usage for power users\n- Must maintain backward compatibility through proper versioning and deprecation cycles\n- Must expose QF's unique features, e.g. 0.1s block time with appropriate transaction confirmation UX patterns; SPIN consensus indicators showing different finality levels (fast local vs. relay chain)\n- API design must follow established patterns from widely-adopted blockchain SDKs to reduce learning curve\n- Must use consistent naming patterns and API design that align with the broader QF ecosystem\n\n## Risks\n\n- Risk: WebRTC connectivity presents challenges in certain network environments (corporate firewalls, VPNs)\n  Mitigation: Implement detailed connection diagnostics, clear error messages with troubleshooting steps, and graceful fallback options\n\n- Risk: Different wallet extensions implement inconsistent interfaces and behaviors\n  Mitigation: Build a robust abstraction layer with extension-specific adapters and comprehensive testing across wallet types\n\n- Risk: Wallet extensions may introduce breaking changes in their APIs\n  Mitigation: Implement version detection and maintain compatibility layers for major wallet extension versions\n\n- Risk: Big number handling and token decimal calculations could lead to subtle bugs\n  Mitigation: Implement comprehensive test suite focused on edge cases in financial calculations and provide detailed examples of correct token amount handling\n\n- Risk: TypeScript type definitions might not fully capture all possible blockchain responses\n  Mitigation: Generate types from chain metadata where possible and implement runtime type checking to provide helpful error messages when unexpected data is encountered\n\n- Risk: Complex pagination requirements for large data sets like transaction history\n  Mitigation: Implement standard pagination patterns and consider integrating with or recommending indexing solutions for historical data access\n\n- Risk: Developers might encounter rate limiting when connecting through public RPC endpoints\n  Mitigation: Implement transparent rate limit handling, queuing, and provide documentation on self-hosting nodes for production applications\n\n- Risk: Large transaction batches could overwhelm client resources\n  Mitigation: Implement progressive chunking strategies with appropriate backpressure handling and progress reporting\n\n- Risk: Session-based transactions could introduce security vulnerabilities if implemented incorrectly\n  Mitigation: Implement time-based and scope-based limitations, with clear security boundaries and automated testing for potential attack vectors\n\n- Risk: S3 interface compatibility might be challenging to maintain with blockchain storage limitations\n  Mitigation: Clearly document any compatibility limitations and provide fallback mechanisms for unsupported features\n\n- Risk: Simulated blockchain environment may not perfectly match production behavior\n  Mitigation: Provide both simulation and live-chain testing options, with clear documentation of limitations\n\n- Risk: Adopting established patterns might conflict with QF-specific optimizations\n  Mitigation: Create extension points and alternative interfaces where necessary while maintaining familiar baseline patterns\n\n- Risk: npm package name collision.\n  Mitigation: reserve `@qf/web-sdk` scope before public announcement.\n\n## Development Outcomes\n\n- A fully-functional TypeScript Web SDK published on npm with semantic versioning and comprehensive release notes\n- SDK core functionality including:\n  - WebRTC connectivity with the QF blockchain\n  - Account and wallet management with support for multiple wallet extensions\n  - Transaction handling with proper error management and severity levels\n  - Smart contract deployment and interaction with Web2-familiar patterns\n  - State synchronization for light clients\n  - Transaction lifecycle tracking system with filterable events\n  - Session-based transaction system for recurring operations\n  - S3-compatible storage interfaces\n  - History querying with flexible filtering\n- SDK utility features:\n  - Big number and token decimal handling with formatter utilities\n  - Automatic retry mechanisms for failed requests\n  - Informative error messages with suggested solutions\n  - Efficient pagination implementation for large data queries\n  - Helper methods for common operations\n  - Batched transaction handling optimized for large volumes\n  - Counterfactual transaction simulation\n  - Periodic and streaming payment interfaces\n- Performance benchmarks comparing QF Web SDK efficiency with competing SDKs\n- Starter kit templates demonstrating SDK integration in common application patterns\n- (optional) Testing framework for blockchain applications:\n  - Transaction simulation without live blockchain\n  - State assertion and verification utilities\n  - Time manipulation for testing time-dependent logic\n  - Mock wallet providers\n  - Network condition simulators\n\n## Decision Log\n\n[To be filled during development]\n\n## Notes\n\n- Consider exploring language bindings beyond TypeScript (Python, Rust, Go) based on community demand\n- Investigate Web Worker support for computationally intensive operations\n- Regular user testing with both blockchain-experienced and blockchain-new developers will be crucial to validate the SDK's usability\n- Create a roadmap for advanced features like AI-assisted code generation for QF applications\n- Consider integration with popular IDEs through plugins/extensions for improved developer experience\n- Explore implementation of SDK telemetry to identify common usage patterns and pain points (with opt-in)\n- Investigate progressive enhancement strategies for users in bandwidth-constrained environments",
                "url": "https://github.com/QuantumFusion-network/spec/issues/334",
                "comments": []
            },
            {
                "number": 306,
                "title": "Developer Portal implementation",
                "state": "open",
                "author": "actinfer",
                "labels": [
                    "Drafted"
                ],
                "created_at": "2025-04-22T05:36:40Z",
                "updated_at": "2025-05-29T11:30:05Z",
                "closed_at": null,
                "body": "This case focuses on developing and deploying a comprehensive developer ecosystem for the Quantum Fusion Network. By creating a public developer portal with documentation and tools, auto-generated code documentation, and related resources, we will establish the primary entry points for developers to understand, test, and deploy applications on QF.\nThe developer ecosystem will follow established industry patterns while providing a distinctive QF experience that accelerates adoption and improves developer satisfaction. This initiative is crucial as it directly impacts adoption rates and developer experience, making it a key factor in the success of QF blockchain adoption.\n\n## System Overview\n\n1. **Discovery Portal**: Public-facing website with learning paths, search, and navigation\n2. **Documentation System**: Content management, versioning, and publishing infrastructure\n3. **Interactive Tools**: API playground, code editors, and testing environments\n4. **Developer Intelligence**: Analytics, telemetry, and feedback collection systems\n5. **Community Platform**: Contribution workflows, support channels, and governance forums\n\n## Expected Behaviors\n\n### Discovery Portal System\n- Adapts entry points based on developer background (Ethereum developers see Solidity comparison)\n- Provides personalized learning paths that adjust based on progress and interests\n- Surfaces relevant content based on previous interactions and detected knowledge gaps\n- Highlights community success stories and production use cases to build confidence\n- Maintains consistent branding and navigation patterns with the main QF website\n\n### Documentation System\n- Organizes content in four distinct layers matching learning progression:\n1. **Conceptual layer**: High-level explanations and architecture overviews for beginners\n2. **Practical layer**: Step-by-step guides and tutorials for hands-on learning\n3. **Reference layer**: Comprehensive API and function documentation for implementation\n4. **Advanced layer**: Optimization techniques and system internals for experts\n- Automatically indicates content freshness with last-updated timestamps and version badges\n- Maintains version-specific documentation for all supported API and runtime versions\n- Preserves permalinks to specific documentation versions for reference stability\n- Allows switching between versions with visible indicators of breaking changes\n\n### Interactive Tools System\n- Provides browser-based playground requiring zero installation for initial testing\n- Maintains synchronized state between code examples and API documentation\n- Saves developer sessions and experiments for continuation across devices\n- Offers downloadable code samples that are guaranteed to work with current API versions\n- Includes project scaffolding tools that generate starter templates with best practices\n\n### Developer Intelligence System\n- Tracks developer journey metrics while respecting privacy preferences\n- Triggers contextual help based on detected friction points (e.g., repeated failed API calls)\n- Generates daily reports on documentation usage, search terms, and common pain points\n- Visualizes developer cohort progression from onboarding through production deployment\n- Measures developer satisfaction through unobtrusive, contextual micro-surveys\n\n### Community Platform System\n- Facilitates direct contribution to documentation with appropriate review processes\n- Recognizes and rewards community contributions through visible attribution\n- Provides structured templates for reporting issues or requesting enhancements\n- Connects developers with similar interests or complementary skills\n- Enables direct engagement with QF core team members for strategic questions\n\n## Stakeholders and User Flows\n\n### Solo Developers & Hobbyists\n- **Onboarding Flow**: Discovers QF through search -> Views conceptual overview -> Accesses quick start guide -> Uses one-click setup -> Deploys first contract within 10 minutes\n- **Needs**: Frictionless setup, clear tutorials, playground environment, minimal requirements\n- **Success Indicators**: Completes first deployment, returns within 7 days, attempts second project\n\n### dApp Development Teams  \n- **Integration Flow**: Evaluates QF capabilities -> Tests API compatibility -> References migration guides -> Integrates with existing systems -> Deploys and monitors production applications\n- **Needs**: Comprehensive API reference, migration paths, production best practices, troubleshooting tools\n- **Success Indicators**: Successfully migrates existing dApps, reduces development time by 30%\n\n### Protocol Researchers\n- **Research Flow**: Accesses technical specifications -> Reviews consensus mechanisms -> Analyzes performance characteristics -> Downloads whitepapers -> Incorporates findings into publications\n- **Needs**: Detailed technical explanations, architectural diagrams, formal specifications, benchmarking data\n- **Success Indicators**: Cites QF in research, participates in protocol governance discussions\n\n### Node Operators & Validators\n- **Operations Flow**: Reviews hardware requirements -> Downloads installation package -> Follows step-by-step setup guide -> Configures validation parameters -> Monitors network performance\n- **Needs**: Clear installation instructions, security practices, troubleshooting guides, telemetry setup\n- **Success Indicators**: Successfully runs validator node, maintains >99% uptime, participates in upgrades\n\n### Enterprise Integration Teams\n- **Enterprise Flow**: Assesses compliance requirements -> Reviews security documentation -> Evaluates integration options -> Implements secure connections -> Gradually scales deployment\n- **Needs**: Security documentation, enterprise patterns, SLA information, direct support channels\n- **Success Indicators**: Completes enterprise integration, becomes reference customer, extends usage\n\n### QF Team Members (New Hires)\n- **Onboarding Flow**: Accesses internal onboarding documentation -> Follows environment setup guides -> Studies architecture overview -> Completes guided development tasks -> Contributes to codebase within first week\n- **Needs**: Clear development workflows, architecture diagrams, coding standards, troubleshooting guides\n- **Success Indicators**: Completes first code contribution within one week, understands system architecture\n\n### AI Documentation Assistant\n- **Generation Flow**: Monitors code repository changes -> Analyzes code comments and structure -> Generates or updates documentation -> Submits for human review -> Incorporates feedback for future improvements\n- **Needs**: Access to code repositories, engineering specifications, existing documentation\n- **Success Indicators**: Reduces documentation creation time by 50%, maintains 90% approval rate in reviews\n\n## Content Sources\n\n1. **Engineering Specifications**\n   - Formal technical designs\n   - Architecture diagrams and system interactions\n   - Performance characteristics and constraints\n   - Protocol definitions and standards\n\n2. **Auto-Documentation**\n   - Rustdoc and similar tools for API reference generation\n   - Code comment extraction and formatting\n   - Type definitions and function signatures\n   - Version-specific API changes and compatibility notes\n\n3. **AI-Generated Documentation**\n   - Automated dev wiki updates following code changes\n   - Personalized onboarding paths based on developer profiles\n   - Product descriptions and feature summaries\n   - Usage examples and code snippets\n   - First drafts of tutorials and guides for human refinement\n\n## Integration Points Between Subsystems\n\n### Discovery Portal - Documentation System\n- Portal pages embed live rustdoc content via iframes or direct links\n- Code examples in tutorials automatically pull from tested repository examples\n- API changes trigger portal content review workflows\n- AI-generated content previews appear in portal for early feedback\n\n### Discovery Portal - Interactive Tools\n- Tutorial pages include \"Try this code\" buttons that open playground with pre-filled examples\n- Playground results can be saved and shared via portal URLs\n- Failed playground attempts generate improvement suggestions for tutorials\n- Learning pathways adjust based on playground success patterns\n\n### Discovery Portal - Developer Intelligence\n- Page view data identifies most/least useful content\n- User journey tracking shows where developers get stuck\n- A/B testing of different explanation approaches\n- Content recommendations based on developer progress and preferences\n\n### Documentation System - AI Assistant\n- Code repository changes trigger AI documentation updates\n- AI drafts undergo human review workflow before publication\n- Human edits train AI to improve future documentation\n- AI flags outdated documentation based on code divergence\n\n### Interactive Tools - Developer Intelligence\n- Playground error patterns reveal common developer mistakes\n- Tool usage metrics inform feature prioritization\n- Successful code patterns become recommended examples\n- Abandoned sessions trigger targeted help interventions\n\n### Community Platform - All Subsystems\n- Portal feedback improves tutorial clarity\n- Playground frustrations update code examples\n- Documentation gaps trigger rustdoc comment improvements\n- Community contributions receive public attribution\n- Community votes influence documentation priorities\n\n### Cross-System Developer Journey\n- Developer creates account → tracked across all subsystems\n- Playground usage data → improves tutorial examples → measured in portal analytics\n- API documentation views → inform playground pre-loaded examples → tracked in metrics\n- Failed API calls → trigger contextual help → inform documentation improvements\n\n### Shared Infrastructure\n- Single sign-on across all subsystems\n- Unified search that covers all ecosystem content\n- Consistent design system and branding\n- Shared content management for cross-references\n- Integrated feedback collection at all touchpoints\n\n### Public Ecosystem - Internal Documentation\n- Technical specifications flow from internal docs to public tutorials\n- Public feedback informs internal documentation improvements\n- New feature development in internal docs becomes public documentation after release\n- Internal team training materials adapt for public consumption where appropriate\n\n## Breaking Change Management\n\n1. **Advance Notice**: Breaking changes announced at least 60 days before implementation\n2. **Change Classification**: Changes categorized by impact level (Minor, Major, Critical)\n3. **Migration Tooling**: Automated tools provided for code updates when possible\n4. **Deprecation Markers**: Deprecated features clearly marked in documentation and IDE plugins\n5. **Transition Period**: Overlapping support for old and new APIs during migration windows\n6. **Notification System**: Email alerts for registered developers affected by relevant changes\n7. **Change Logs**: Detailed, searchable logs of all changes with impact assessments\n8. **Version-Specific Documentation**: Historical documentation preserved for legacy support\n\n## Learning Curve Alignment\n\n1. **Newcomer Stage** (~2 days)\n   - Conceptual overviews with minimal technical jargon\n   - Getting started guides with one-click setup\n   - Simple examples with complete code samples\n   - Guided tutorials with expected outcomes\n\n2. **Beginner Stage** (~3-14 days)\n   - Fundamental concepts explained in depth\n   - Common patterns and use cases\n   - Troubleshooting guides for frequent issues\n   - Interactive exercises with increasing complexity\n\n3. **Intermediate Stage** (~15-30 days)\n   - Component integration patterns\n   - Performance optimization techniques\n   - Security best practices\n   - Advanced API usage scenarios\n\n4. **Advanced Stage** (~31+ days)\n   - System architecture deep dives\n   - Custom extension development\n   - Scaling and production deployment\n   - Contributing to core protocol\n\n## Development Approach\n\n1. **Hypothesis**: We believe providing a 10-minute onboarding experience will increase developer retention by 30%\n   - **Experiment**: Create streamlined quick start with one-click setup\n   - **Measurement**: Track completion rates and 7-day return visits\n   - **Learning**: Refine based on dropout points and feedback\n\n2. **Hypothesis**: We believe contextual documentation will reduce support requests by 40%\n   - **Experiment**: Implement context-aware help system\n   - **Measurement**: Compare support ticket volume before/after\n   - **Learning**: Identify remaining knowledge gaps\n\n3. **Hypothesis**: We believe interactive examples will increase API adoption by 25%\n   - **Experiment**: Create embedded API playground\n   - **Measurement**: Track playground usage to production API calls\n   - **Learning**: Identify which examples lead to implementation\n\n4. **Hypothesis**: We believe AI-generated documentation can reduce documentation creation time by 50%\n   - **Experiment**: Implement AI documentation pipeline for code changes\n   - **Measurement**: Compare time to document features with/without AI\n   - **Learning**: Identify areas where AI excels or struggles\n\n5. **Hypothesis**: We believe structured team onboarding documentation will reduce time-to-productivity for new hires by 40%\n   - **Experiment**: Create guided onboarding path with checkpoints for new team members\n   - **Measurement**: Track time to first contribution before/after implementation\n   - **Learning**: Identify remaining onboarding friction points\n\n- **AI Contribution**: 60% of routine documentation updates handled by AI with minimal human oversight\n\n## Related Work\n- Developer portal examples: [Stripe](https://stripe.com/docs), [Twilio](https://www.twilio.com/docs)\n- Blockchain developer experiences: [Ethereum.org](https://ethereum.org/developers/), [docs.polkadot.com](https://docs.polkadot.com/)\n- Documentation frameworks: [Diátaxis](https://diataxis.fr/), [Rust API Guidelines](https://rust-lang.github.io/api-guidelines/)\n- Developer experience research: [State of Developer Experience Reports](https://getdx.com/)\n\n## Constraints\n- Must work reliably across all major browsers and provide reasonable mobile experience\n- All code examples must stay current with QF versions through automated testing\n- Breaking changes require proactive communication with migration paths before release\n- System must handle 1000+ concurrent developers during peak usage\n\n## Risks\n- Subsystems evolve inconsistently\n  - Mitigation: Shared design system and regular cross-team alignment\n- Documentation becomes outdated\n  - Mitigation: Automated testing of all examples and staleness monitoring\n- Too much input without clear prioritization\n  - Mitigation: AI-assisted feedback categorization and impact scoring\n- Integration points become maintenance burden\n  - Mitigation: Clear API contracts between subsystems and independent deployment\n\n## Development Outcomes\n- Five integrated subsystems working together seamlessly\n- Hypothesis validation framework with measurable outcomes\n- Developer journey optimization based on real usage data\n- Community-driven improvement process with clear feedback loops\n- Scalable architecture supporting 1000+ concurrent developers\n\n## Decision Log\n\n\n## Notes\n\n",
                "url": "https://github.com/QuantumFusion-network/spec/issues/306",
                "comments": [
                    {
                        "author": "TriplEight",
                        "created_at": "2025-05-22T16:51:49Z",
                        "body": " cc meta https://github.com/QuantumFusion-network/spec/pull/352\n\n- The portal is described as a monolith instead of a system-of-systems where the portal, codebase, metrics infrastructure, knowledge base, and API playground are distinct but integrated subsystems with lifecycles of their own.\n- Key stakeholders (e.g., solo developers, dApp teams, protocol researchers, devops validators, degens, normies, etc) are not mapped to persona-specific flows, making it hard to validate whether each requirement maps to a core user goal.\n- there is no systemic plan for collecting developer feedback beyond GitHub issues or page edits\n- No mention of monitoring documentation decay, understanding friction points in onboarding, or conducting developer experience (DX) retrospectives. _These would become connectors to community manager's and DevRel's role sets._\n- No mention of how breaking changes, API deprecations, or runtime upgrades will be communicated or mitigated in real-time through the portal.\n- The proposal lists tooling choices like rustdoc and CI-based doc updates, but lacks a systems view of the docops lifecycle. It is unclear how documentation tasks are prioritized, how tech writers/devs contribute, and how changes propagate through the CI/CD pipeline.\n\n\nAction items:\n\n- [ ] let's rewrite the case in the stakeholder/product language. This will require clear definition of the system boundaries.\n- [ ] apply functional decomposition to the developer ecosystem and explicitly define subsystems in separate cases.\n- [ ] Introduce a layered systems model in portal design, making it easier for developers to \"drill down\" or \"zoom out\".\n- [ ] move technicalities into the sub-issues - experiments.\n  - [ ] Define hypothesis-driven product goals (e.g., \"We believe adding a one-click faucet will increase onboarding completion by 30%\").\n  - [ ] Instrument the portal to validate these hypotheses iteratively, not just post-facto.\n- [ ] describe user flows\n- [ ] describe DX feedback loops: use short in-portal surveys, feedback buttons, and telemetry-informed triggers (e.g., abandoned onboarding sessions).\n- [ ] Include metrics beyond usage, like satisfaction, task completion time, and drop-off points.\n- [ ] Align sections with the learning curve (e.g., moving from SDK usage to validator telemetry) and expose integration points explicitly.\n- [ ] Treat governance, upgrades, and deprecations as living processes, with changelogs, migration tooling, and alerts integrated into the developer workflow.\n- [ ] external: Define a **docops lifecycle process**: feature definition → doc outline → code comments → markdown authoring → CI integration → feedback.\n\n"
                    }
                ]
            },
            {
                "number": 280,
                "title": "SPIN Secure Finality",
                "state": "open",
                "author": "AlexLgn",
                "labels": [
                    "Drafted"
                ],
                "created_at": "2025-04-08T14:26:49Z",
                "updated_at": "2025-05-22T10:38:29Z",
                "closed_at": null,
                "body": "This case builds upon the initial SPIN consensus implementation (Genesis Phase) to develop advanced features that will be added via runtime upgrades. After the basic consensus is operational, we need to connect our QF chain to the Polkadot testnet (Paseo) through the parachain protocol.\n\nThis connection creates a hybrid consensus architecture where our QF chain operates with 100ms block times and fast finality, while also benefiting from the strong security guarantees of the Polkadot relay chain. The parachain connection serves two key purposes: it provides a channel for sending fast chain finality proofs, and it enables recovery mechanisms if the primary leader fails.\n\nTo establish this connection, we must implement Paseo Bulk Coretime integration, develop recovery mechanisms for handling leader failures, and create secondary leader functionality to ensure continuous operation even during disruptions.\n\n## Objectives\n\n- Establish the hybrid consensus architecture connecting QF chain to Paseo through parachain protocol.\n- Implement Paseo Bulk Coretime integration.\n- Develop recovery mechanisms for handling leader failures.\n- Implement recovery procedures and secondary leader functionality.\n- (Optional) Determine optimal leader rotation timing through empirical testing.\n- (Optional) Determine optimal time for how long one validator should continuously produce blocks.\n\n## Acceptance Criteria\n\n- [ ] In case of a primary leader failure, a secondary leader continues block production without normal transactions inclusion, but obtaining fast chain and secure finality.\n- [ ] Secure finality from Paseo is achieved within 30 seconds, as shown in Polkadot/Substrate Portal.\n- [ ] Fast chain finality proofs (block hash + notarizations) are sent to the parachain, verifiable through the parachain on-chain data.\n- [ ] The most recent securely finalized block header is available via the fast chain RPC method.\n- [ ] Recovery mode activates automatically within a predetermined time-frame when fast chain finality proofs are missing from the parachain for the configured period.\n- [ ] During recovery mode, committee nodes successfully post the latest block hash and notarization to the parachain, verifiable through the parachain on-chain data.\n- [ ] (Optional) Committee accounts failing to post notarizations during recovery are automatically removed from the set.\n- [ ] New primary leader selection occurs automatically at the end of recovery.\n- [ ] The primary new leader successfully resumes block production on top of the block produced by the secondary leader in the end of the recovery mode.\n- [ ] (Optional) Testing data demonstrates optimal leader rotation timing balancing security, scalability, and performance.\n- [ ] (Optional) Analysis provides recommended duration for continuous block production by a single validator based on security and performance testing.\n\n## Related Work\n\n* [Onboarding Parachains - Coretime](https://github.com/paseo-network/paseo-action-submission/blob/main/pas/PAS-10-Onboard-paras-coretime.md) - Required for integration with Paseo\n* [Substrate Consensus Documentation](https://docs.substrate.io/reference/how-to-guides/consensus/)\n* [Thunderella Paper](https://eprint.iacr.org/2017/913.pdf)\n* [Polkadot Documentation](https://wiki.polkadot.network)\n* [Polkadot SDK Repository](https://github.com/paritytech/polkadot-sdk)\n* [Polkadot Reference Hardware specifications](https://wiki.polkadot.network/docs/maintain-guides-how-to-validate-polkadot#reference-hardware)\n\n## Constraints\n\n- Must build upon the Genesis Phase implementation without breaking existing functionality. \n- Must implement Bulk Coretime for Paseo Testnet integration according to Paseo onboarding guide.\n- Must function on standard hardware with specifications matching Polkadot reference hardware.\n- Must maintain compatibility with Polkadot relay chain.\n- Must provide appropriate finality data that can later be displayed in block explorer (block explorer modifications are covered in a [QF Ecosystem Portal, Management and Development Tools](https://github.com/QuantumFusion-network/spec/issues/34) case).\n\n## Risks\n\n- Security vulnerabilities in the consensus implementation could allow attacks\n  - Mitigation: Review implementation against security best practices and conduct code reviews\n\n- Improper security measures could create exploit vectors\n  - Mitigation: Follow Polkadot security recommendations and validate security assumptions\n\n- Integration with Paseo might break if Polkadot makes incompatible SDK/API changes\n  - Mitigation: Monitor Polkadot updates and maintain regular communication with the Paseo team\n\n- Communication between fast chain, parachain, relay chain might fail or be delayed\n  - Mitigation: Implement robust error handling and recovery mechanisms for inter-layer communication\n\n- Infrastructure for running the nodes might not be properly set up\n  - Mitigation: Ensure consensus implementation is compatible with standard deployment practices\n\n- Recovery mechanisms might not activate properly or could result in chain stalls\n  - Mitigation: Extensively test recovery scenarios with simulated leader failures\n\n- Timeline risks may impact QF development roadmap and adoption\n  - Mitigation: Prioritize core functionality implementation and set clear milestones\n\n## Development Outcomes\n\n- Complete hybrid consensus architecture connecting QF chain to Paseo\n- Functional recovery mode system that automatically handles network issues\n- Secondary leader implementation for continuous operation during primary leader failures\n- Integration with Paseo testnet following their onboarding procedures\n- Performance benchmark report showing consensus operation under different conditions\n- Technical documentation covering:\n  - Advanced recovery mechanisms\n  - Hybrid consensus architecture\n  - Paseo integration details\n  - (Optional) Optimal leader rotation timing recommendations\n  - (Optional) Analysis of ideal validator block production duration\n\n## Decision Log\n\n## Notes\n[Terms and definitions](https://github.com/QuantumFusion-network/spec/blob/main/docs/SPIN/testnet_features_set.md)",
                "url": "https://github.com/QuantumFusion-network/spec/issues/280",
                "comments": []
            },
            {
                "number": 258,
                "title": "Infrastructure resilience",
                "state": "open",
                "author": "khssnv",
                "labels": [
                    "Drafted"
                ],
                "created_at": "2025-03-28T07:39:33Z",
                "updated_at": "2025-06-03T15:51:34Z",
                "closed_at": null,
                "body": "This case focuses on building robust infrastructure systems that can reliably support increased developer activity during Testnet launch. By implementing comprehensive monitoring, security hardening, and automated recovery systems, we will ensure that QF infrastructure can handle production-level traffic while maintaining high availability for developers and users.\n\nThe infrastructure resilience initiative addresses the critical need for stable, secure, and observable systems as QF transitions from development to public Testnet. This directly impacts developer experience, user trust, and operational efficiency during the crucial Testnet phase.\n\n## Objectives\n\n- **Hypothesis**: We believe that implementing comprehensive infrastructure monitoring will reduce system downtime by 80% and enable proactive issue resolution\n- **Hypothesis**: We believe that standardizing deployment processes will reduce deployment-related incidents by 90% and enable faster feature delivery\n- **Hypothesis**: We believe that implementing automated backup and recovery systems will reduce node synchronization time from hours to minutes\n- **Hypothesis**: We believe that security hardening will prevent infrastructure compromises and ensure developer trust during Testnet launch\n- **Hypothesis**: We believe that if external developers see a network with visible uptime guarantees, self-healing nodes and clear incident history, they will start building dApps 30% faster\n\n## Expected Behaviors\n\n### Monitoring and Observability System\n- Automatically detects performance degradation before it impacts users\n- Provides real-time visibility into blockchain node health, synchronization status, and consensus issues\n- Generates actionable alerts for infrastructure teams with clear resolution paths\n- Displays public status information that developers can check independently\n- Tracks key metrics (block time, node sync speed, RPC response times) with historical trends\n\n### Security and Hardening System\n- Prevents unauthorized access to critical infrastructure components\n- Ensures all deployed binaries are cryptographically signed and verified\n- Maintains audit trails of system changes and access attempts\n- Isolates services using network segmentation and access controls\n- (Optional) Automatically detects and responds to sophisticated security threats with advanced monitoring\n\n### Backup and Recovery System\n- Creates automatic blockchain database snapshots at regular intervals\n- Enables new nodes to synchronize quickly using recent snapshots instead of full chain replay\n- Provides reliable disaster recovery with tested restoration procedures\n- Supports external node operators with easily accessible snapshot downloads\n\n### Deployment and Configuration System\n- Standardizes service configurations across development, testing, and production environments\n- Automatically invalidates caches and updates configurations during deployments\n- Prevents configuration drift through centralized management\n- Enables rapid scaling of infrastructure components when needed\n\n### Service Management System\n- Orchestrates blockchain nodes, databases, indexers, and web applications consistently\n- Provides service discovery and health checking across all components\n- Enables zero-downtime deployments and rolling updates\n- Maintains high availability through automated failover mechanisms\n- (Optional) Advanced container orchestration with automatic resource optimization\n\n### Performance Testing System\n- (Optional) Validates system capacity under realistic load conditions\n- (Optional) Identifies performance bottlenecks before they impact users\n- (Optional) Provides baseline metrics for scaling decisions\n\n## Stakeholders and User Flows\n\n### Observability Platform\n- **Flow**: Monitoring system polls infrastructure components → Stores time-series data → Alert manager triggers on rule violations → Dashboard displays metrics → Status page shows public health\n- **Needs**: Reliable metric collection, fast alert delivery, comprehensive visualization, public transparency\n- **Success Indicators**: <1% monitoring failures per day, 95% of alerts within 30s, no gaps >5min in critical metrics\n\n### Public RPC Gateway Cluster\n- **Flow**: Poll full nodes for latest blocks → Cache responses → Serve JSON-RPC & WebSocket requests → Monitor latency/errors → Failover via health-checks\n- **Needs**: Fresh blockchain data, low latency responses, automatic failover, comprehensive monitoring\n- **Success Indicators**: Max 1 block behind tip 99.9% of time, p99 < 300ms, 99.9% monthly uptime\n\n### Backup and Snapshot System\n- **Flow**: Schedule automated snapshots → Compress databases → Upload to distributed storage → Validate integrity → Distribute via CDN\n- **Needs**: Regular scheduling, efficient compression, reliable storage, integrity verification, fast distribution\n- **Success Indicators**: <30 minutes snapshot creation, >50MB/s download speed, 99.95% availability\n\n### Security Monitoring System\n- **Flow**: Collect logs from all components → Parse and normalize → Apply threat detection rules → Alert incident response team → Maintain audit trails\n- **Needs**: Comprehensive log collection, real-time analysis, accurate threat detection, incident escalation, compliance reporting\n- **Success Indicators**: <5 second processing delay, <2% false positive rate, 100% component coverage\n\n### Service Orchestration Platform\n- **Flow**: Container scheduler deploys services → Service registry maintains discovery → Secret manager distributes credentials → Monitor resource utilization → Auto-scale based on demand\n- **Needs**: Container orchestration, service discovery, secret management, health monitoring, automatic scaling\n- **Success Indicators**: 99.9% service uptime, >95% first-attempt deployment success, <1% false health check failures\n\n### Configuration Management System\n- **Flow**: Generate standardized configuration templates → Validate configuration syntax → Version control changes → Deploy configs via automation → Monitor for drift\n- **Needs**: Standardized templates, syntax validation, version control, automated deployment, drift detection\n- **Success Indicators**: 0 manual config edits per environment, 100% validation before deployment, <5 minute propagation time\n\n### (Optional) Load Testing System\n- **Flow**: Schedule performance tests → Generate realistic traffic patterns → Monitor system response under load → Generate performance reports → Trigger scaling decisions\n- **Needs**: Realistic traffic simulation, performance baselines, automated scheduling, clear reporting, capacity planning\n- **Success Indicators**: Tests complete within 30 minutes, identify bottlenecks before 80% capacity, zero false performance alerts\n\n### Blockchain Node Cluster\n- **Flow**: Maintain synchronized state → Generate blocks per consensus → Validate incoming blocks → Update local state → Serve data to RPC gateways\n- **Needs**: Fast synchronization, reliable block production, consensus participation, state management, data serving\n- **Success Indicators**: 30 minute sync to tip with snapshots, <100ms block time variance, >99% validator uptime\n\n### Infrastructure/DevOps Team\n- **Deployment Flow**: Push tag → CI builds/signs/publishes → Container platform deploys → CDN cache purge → Health checks pass → Team notification system reports success\n- **Incident Flow**: Receives alert → Accesses centralized dashboard → Identifies root cause through metrics → Applies fix → Validates resolution through monitoring\n- **Needs**: Automated pipelines, binary signing, cache invalidation, deployment validation, clear alerting, comprehensive dashboards, standardized procedures, audit trails\n- **Success Indicators**: 0 manual steps per release, 100% cache purge success rate, <10 minute deployment cycles, 100% binary signature verification, reduces mean time to resolution by 70%\n\n### Developer Teams (QF Core)\n- **Development Flow**: Commits code → Automated deployment pipeline → Configuration updates applied → Service health validated → Cache invalidation completed\n- **Needs**: Reliable deployments, consistent environments, fast feedback loops\n- **Success Indicators**: Zero deployment failures, 5-minute deployment cycles, consistent environments\n\n### External Node Operators\n- **Setup Flow**: Downloads node software → Accesses latest snapshot → Synchronizes from snapshot → Validates chain state → Begins normal operation\n- **Needs**: Fast synchronization, reliable snapshots, clear setup instructions\n- **Success Indicators**: Synchronization completes in under 30 minutes, 99% snapshot availability, 95% of first-time node setups succeed without support ticket\n\n### Testnet Developers\n- **Usage Flow**: Connects to QF RPC → Submits transactions → Receives confirmations → Monitors application performance → Reports issues if needed\n- **Needs**: Stable RPC endpoints, consistent performance, transparent status information\n- **Success Indicators**: 99.9% RPC uptime, sub-second response times, clear incident communication\n\n### (Optional) Security Auditors\n- **Review Flow**: Accesses audit logs → Reviews security configurations → Validates compliance → Identifies vulnerabilities → Provides recommendations\n- **Needs**: Comprehensive logging, security documentation, vulnerability scanning results\n- **Success Indicators**: Zero critical vulnerabilities, compliance with security standards\n\n### QF Stakeholders\n- **Oversight Flow**: Reviews infrastructure metrics → Assesses operational costs → Monitors uptime statistics → Makes scaling decisions → Communicates with team and other stakeholders\n- **Needs**: High-level dashboards, cost visibility, performance summaries, incident reports\n- **Success Indicators**: Infrastructure costs within budget, meets SLA commitments, supports business growth\n\n## Related Work\n\n- Infrastructure monitoring: [Grafana](https://grafana.com/), [Prometheus](https://prometheus.io/), [DataDog](https://www.datadoghq.com/)\n- Public status pages: [Kener](http://kener.ing/), [Uptime Robot](https://uptimerobot.com/)\n- Security frameworks: [NIST Cybersecurity Framework](https://www.nist.gov/cyberframework), [CIS Controls](https://www.cisecurity.org/controls)\n- Blockchain infrastructure: [Ethereum node operation](https://ethereum.org/en/developers/docs/nodes-and-clients/run-a-node/), [Polkadot validator guide](https://docs.polkadot.com/infrastructure/running-a-validator/onboarding-and-offboarding/set-up-validator/)\n- Service orchestration: [HashiCorp Nomad](https://www.nomadproject.io/), [Consul](https://www.consul.io/), [Vault](https://www.vaultproject.io/)\n- Security monitoring: [Wazuh](https://wazuh.com/), [OSSEC](https://www.ossec.net/)\n- Load testing: [Artillery](https://artillery.io/), [k6](https://k6.io/), [JMeter](https://jmeter.apache.org/)\n\n## Constraints\n\n- Must maintain compatibility with existing QF node software and protocols\n- Must keep 100ms block-time goal intact\n- Security implementations must not significantly impact performance or user experience\n- Backup systems must not interfere with normal blockchain operations\n- All monitoring must respect user privacy and avoid collecting sensitive data\n- Infrastructure changes must support both current DevNet and future Testnet requirements\n\n## Risks\n\n- Monitoring overhead could impact node performance\n  - Mitigation: Use lightweight agents and optimize data collection frequency\n- Security hardening might break existing integrations\n  - Mitigation: Implement changes gradually with rollback procedures and thorough testing\n- Backup processes could consume excessive storage or bandwidth\n  - Mitigation: Implement compression, incremental backups, and retention policies\n- Service standardization might introduce single points of failure\n  - Mitigation: Design redundancy into standardized configurations and maintain failover options\n- Single-vendor lock-in with CDN provider\n  - Mitigation: CDN can front traffic, but origin infra stays portable\n- Unsigned binary slips into production\n  - Mitigation: CI gate + mandatory signature verification before deployment\n- Alert fatigue reduces incident response effectiveness\n  - Mitigation: Sane SLO thresholds, grouped routing, weekly rules review\n- Load testing could impact production systems\n  - Mitigation: Isolated testing environments, traffic shaping, automated safeguards\n- Complex deployment pipelines could slow down development velocity\n  - Mitigation: Focus on automation and provide manual override capabilities for emergencies\n\n## Development Outcomes\n\n- Comprehensive monitoring system displaying real-time infrastructure health with automated alerting for critical issues\n- Public status page providing transparent communication about system availability and ongoing incidents  \n- Automated backup and snapshot system enabling rapid node synchronization and disaster recovery\n- Security hardening implementation including binary signing, access controls, and intrusion detection\n- Standardized deployment pipeline with environment consistency and automated configuration management\n- Service orchestration platform managing all infrastructure components with service discovery and health checking\n- Performance optimization tools for identifying and resolving blockchain node synchronization bottlenecks\n- Incident response procedures with defined escalation paths and communication protocols\n- Infrastructure documentation covering setup, maintenance, and troubleshooting procedures\n- Automated cache invalidation system ensuring zero stale content after deployments with CDN API integration\n- Standardized configuration templates eliminating copy-paste errors and configuration drift across environments\n- (Optional) Load testing framework validating system performance under Testnet traffic conditions with automated capacity planning\n- (Optional) Cost monitoring and optimization system tracking infrastructure expenses and identifying reduction opportunities\n- (Optional) Advanced security monitoring with comprehensive audit trails and threat detection\n\n## Decision Log\n\n[Record key decisions made during the case]\n\n## Notes\n\n",
                "url": "https://github.com/QuantumFusion-network/spec/issues/258",
                "comments": [
                    {
                        "author": "dedok",
                        "created_at": "2025-05-22T12:11:17Z",
                        "body": "Add this notes here:\nVas Soshnikov: For Mainnet:\n(1) Add Crypto Server for signing & checking binaries. First ADR (since it's really important), then implementation. \n@Denis Pisarev amend: use manual signature on the bare-metal build machine for official production releases\nThis security practice can not be automated.\n(2) Add a workflow to the repos to check sources, that could help to protect to have out-of-date services, etc.\n(3) Idea from @Denis Pisarev introduce tools for log analyzing and emit events based on it (audit log analyzer or SIEM)Vas Soshnikov: Task to @Alexei Karyagin demonstrate killing of the node & alert triggering to @Kirill Pimenov via screencast"
                    },
                    {
                        "author": "khssnv",
                        "created_at": "2025-05-29T07:30:20Z",
                        "body": "Initial ideas from @akablockchain2, @AlexLgn, @khssnv.\n\n- Hashicorp stack.\n    - Nomad - normalize and regulate services (blockchain nodes, databases, indexer, web apps).\n    - Consul - service discovery / mesh.\n    - Vault - additional security, automated.\n- Cybersecurity.\n    - Systems hardening, container images hardening, networks hardening (VPN to avoid services public exposure).\n    - SIEM (system integrity check), abuse detection, blockchain anomalies and bots detection (Wazuh).\n    - Smart contracts security - transactions, events, system calls listener; bots detector; abuse DB checker for RPC client (Wazuh with a custom extensions).\n- Observability improvement.\n    - Unify observability services.\n    - Achieve larger observability coverage (add databases, services observability).\n    - High quality RPC monitoring with degradation detection, synthetic tests, manual scaling and autoscaling.\n    - Public status page.\n- Backups.\n    - Blockchain databases snapshots facility (required for (auto)scaling and external nodes operators support).\n\nWIP"
                    }
                ]
            },
            {
                "number": 250,
                "title": "Smart contract environment in Mainnet",
                "state": "open",
                "author": "khssnv",
                "labels": [
                    "Conceived"
                ],
                "created_at": "2025-03-25T12:33:39Z",
                "updated_at": "2025-04-03T10:49:24Z",
                "closed_at": null,
                "body": "[Provide a concise description of the case, including its context and significance to the project]\n\n## Objectives\n- [Objective 1]\n- [Objective 2]\n- [Objective 3]\n\n## Acceptance Criteria\n- [ ] [Criterion 1]\n- [ ] [Criterion 2]\n- [ ] [Criterion 3]\n\n## Related Work\n- [Link to related cases, experiments, or tasks]\n- [Link to relevant documentation or research]\n\n## Constraints\n- [List any known constraints or limitations]\n\n## Risks\n- [Identify potential risks and their impact]\n\n## Learning Outcomes\n[To be filled during and after case completion]\n\n## Decision Log\n[Record key decisions made during the case]\n\n## Notes\n[Any additional information or context]\n",
                "url": "https://github.com/QuantumFusion-network/spec/issues/250",
                "comments": []
            },
            {
                "number": 170,
                "title": "Genesis testnet SPIN consensus implementation",
                "state": "open",
                "author": "khssnv",
                "labels": [
                    "In Use"
                ],
                "created_at": "2025-02-20T15:59:10Z",
                "updated_at": "2025-05-20T09:41:31Z",
                "closed_at": null,
                "body": "This case focuses on implementing the core SPIN consensus protocol for the QF testnet's genesis phase, based on the theoretical foundation established in the \"SPIN Consensus Documentation\" case. We will adapt Substrate's consensus mechanisms (BABE or AURA) to allow a single validator to produce blocks continuously while maintaining GRANDPA for finality.\n\nThis initial implementation achieves 100 ms block time with basic committee validation mechanisms, establishing the foundation for the hybrid consensus architecture. It represents the first step toward connecting the QF chain to the Polkadot testnet (Paseo) relay chain through the parachain protocol.\n\n## Objectives\n\n- Implement the core SPIN consensus with fast 100ms block production.\n- Evaluate and select the most suitable base consensus (BABE or AURA) for modification.\n- Implement committee-based voting for fast finality (notarization).\n- Set up the leader and committee basic mechanism.\n- Establish permissionless accounts set for leader candidates and committee members.\n- Configure leader tenure duration and automatic rotation.\n- Avoid testnet blockchain halt or restart from block 0.\n- Document the evaluation process and implementation architecture.\n\n## Acceptance Criteria\n\n- [x] Blocks are produced every 100 ms (on average) during a leader's tenure, as demonstrated by the leader node logs.\n- [x] Committee voting provides fast finality (notarization) within 10 seconds or less, verifiable through Polkadot/Substrate Portal.\n- [x] Permissionless primary leader candidates and committee accounts set with a minimum size restriction and selection from a list of stakers, verifiable through the fast chain on-chain data.\n- [x] Leader tenure duration is configurable by governance transaction and leader rotation occurs automatically after the configured period ends.\n- [x] Evaluation of BABE vs. AURA as base consensus is completed with justification for selection.\n- [x] Technical documentation covers implementation architecture, configuration parameters, and deployment instructions.\n\n## Related Work\n\n* [Substrate Consensus Documentation](https://docs.substrate.io/reference/how-to-guides/consensus/)\n* [Thunderella Paper](https://eprint.iacr.org/2017/913.pdf)\n* [Polkadot Documentation](https://wiki.polkadot.network)\n* [Polkadot SDK Repository](https://github.com/paritytech/polkadot-sdk)\n* [Polkadot Reference Hardware specifications](https://wiki.polkadot.network/docs/maintain-guides-how-to-validate-polkadot#reference-hardware)\n\n## Constraints\n\n- Must use Substrate primitives for block building and chain validation.\n- Must function on standard hardware with specifications matching Polkadot reference hardware.\n- Must provide appropriate finality data that can later be displayed in block explorer (block explorer modifications are covered in a [QF Ecosystem Portal, Management and Development Tools](https://github.com/QuantumFusion-network/spec/issues/34) case).\n- Must focus on consensus implementation only (infrastructure deployment is covered in a [Infrastructure and Development processes](https://github.com/QuantumFusion-network/spec/issues/113) case).\n\n## Risks\n\n- Performance constraints might prevent achieving 100ms block times consistently\n  - Mitigation: Profile the implementation and optimize critical paths; adjust parameters as needed based on testing\n\n- Potential data validity attacks from malicious nodes if validation is improperly implemented\n  - Mitigation: Implement comprehensive input validation and follow Substrate security patterns\n\n- Node synchronization issues for full and archive nodes due to implementation errors\n  - Mitigation: Test synchronization thoroughly with different node configurations\n\n- Memory or resource usage might exceed reasonable limits on standard hardware\n  - Mitigation: Monitor resource usage and benchmark against reference hardware\n\n## Development Outcomes\n\n- Complete basic SPIN consensus implementation with 10 blocks/second production rate.\n- BABE vs. AURA evaluation report with implementation decision.\n- Functional leader election and committee validation mechanism.\n- Configuration parameters for leader tenure and committee operation.\n- Technical documentation covering:\n  - Implementation architecture\n  - Configuration parameters\n  - Deployment instructions\n\n## Decision Log\n\n## Notes\n[Terms and definitions](https://github.com/QuantumFusion-network/spec/blob/main/docs/SPIN/testnet_features_set.md)\n[Testnet fastchain architecture](https://github.com/QuantumFusion-network/spec/blob/main/docs/SPIN/testnet_architecture.md)\n[Runtime description and configuration parameters](https://github.com/QuantumFusion-network/qf-solochain/blob/main/runtimes/qf-runtime/README.md)\n[Local setup instruction](https://github.com/QuantumFusion-network/qf-solochain/blob/main/docs/executables_and_runtimes.md)\n[Testnet infra architecture](https://github.com/QuantumFusion-network/spec/blob/main/docs/TestNet_infra/test_net.md)\n[BABE vs AURA evaluation](https://github.com/QuantumFusion-network/spec/blob/main/docs/SPIN/BABE%20vs%20AURA.md)",
                "url": "https://github.com/QuantumFusion-network/spec/issues/170",
                "comments": []
            },
            {
                "number": 145,
                "title": "PolkaVM smart contract platform advanced development",
                "state": "open",
                "author": "AlexLgn",
                "labels": [
                    "Accepted"
                ],
                "created_at": "2025-02-11T11:08:22Z",
                "updated_at": "2025-05-21T10:28:50Z",
                "closed_at": null,
                "body": "This case focuses on extending the PolkaVM smart contract platform beyond MVP capabilities, introducing advanced features for production readiness. Key enhancements include EVM compatibility, formal verification, performance optimization, and advanced tooling for developers.\nPolkaVM is a crucial within the QF project, focused on developing a next-generation smart contract platform. It addresses the need for a high-performance, flexible, and versatile smart contract execution environment. This is significant because it aims to enable more complex and efficient dApps on QF, which is critical for mainstream adoption. PolkaVM tackles slow smart contract execution by focusing on:\n    - High-performance execution: Achieving near native-speed using the RISC-V instruction set.\n    - EVM compatibility: Enabling easy migration of existing Solidity codebases.\n    - Flexibility and Versatility: Supporting diverse programming languages and tools.\n    - Optimization: Fine-tuning PolkaVM's performance for low latency and high throughput.\nBy addressing the limitations of current smart contract platforms, PolkaVM is essential to QF's goal of creating a blockchain ready for mass-adopted decentralized application.\n\n## Objectives\n\n- Achieve near-native smart contract execution speed by optimizing RISC-V instruction set implementation.\n- Create an efficient compatibility layer for porting existing Solidity contracts to QF.\n- Implement contract-to-contract call functionality allowing smart contracts to securely interact with each other.\n- Implement formal verification methods for smart contract security.\n- Support multiple programming languages through compiler toolchain.\n- Research and establish optimal gas limits through empirical testing, ensuring contract execution doesn't impact blockchain performance.\n- Design and implement an advanced storage system with:\n  - Storage rent/deposit mechanisms\n  - Optimized storage patterns for frequently accessed data\n  - Advanced CRUD operations with efficient indexing\n- Optimize contract execution performance and resource usage for low latency and high throughput.\n- Create advanced precompiles for common operations (e.g., cryptographic functions, Merkle proof verification).\n- Implement detailed contract interaction analytics and monitoring tools.\n- Develop advanced debugging and testing tools, including a local execution environment\n- Implement a comprehensive smart contract event system for efficient dApp interactions.\n- Develop stability and performance tests for the smart contract platform.\n- Create comprehensive dApp development guide for developers.\n- Adapt and implement [contracts-ui](https://github.com/use-ink/contracts-ui) for interactive smart contract deployment and testing.\n- Migrate pvm-dapp-demo apps to the new smart contracts platform version.\n\n\n## Acceptance Criteria\n\n- [ ] Contract execution performance matches or exceeds major platforms by running standardized benchmark suite comparing execution times with EVM, Wasm, and BPF implementations within 30 minutes.\n- [ ] Contract-to-contract calls work correctly, allowing one smart contract to securely call functions in another contract with proper isolation and error handling.\n- [ ] Solidity contracts successfully deploy and execute without rewriting through compatibility layer by testing standard ERC token contracts within 15 minutes.\n- [ ] Formal verification tools are integrated into the development process to ensure security and successfully identify common vulnerabilities by analyzing test contracts with known issues within 20 minutes.\n- [ ] Multiple language support works correctly by deploying contracts written in different supported languages within 30 minutes.\n- [ ] Gas limits research provides optimal values by measuring execution time of various contract types under different loads and documenting impact on block production.\n- [ ] Contract execution under established gas limits maintains block time by running multiple complex contracts simultaneously and verifying block production remains stable within 5 minutes.\n- [ ] Advanced precompiles execute efficiently by measuring performance improvement over pure contract implementations within 15 minutes.\n- [ ] Precompiled functions (e.g. Merkle proof verification, cryptographic operations) execute correctly by running test suite with various inputs within 10 minutes.\n- [ ] Precompile performance shows improvement by comparing execution time against equivalent pure contract implementations within 15 minutes.\n- [ ] Developer tools enable effective debugging by identifying and fixing common contract issues within 20 minutes.\n- [ ] Contract monitoring provides detailed execution analytics by tracking resource usage and interaction patterns within 10 minutes.\n- [ ] Storage rent/deposit system operates correctly, charging for long-term storage and refunding when storage is freed.\n- [ ]  The smart contract event system successfully emits, indexes, and retrieves events from contract execution.\n- [ ] Apps from pvm-dapp-demo are successfully migrated to the new version of the smart contracts platform and can interact with contracts by submitting transactions with configurable parameters.\n- [ ] User documentation enables successful dApp creation and deployment following a technical guide within 20 minutes without developer assistance.\n- [ ] Stability and performance tests demonstrate the platform can handle multiple contract deployments and interactions without failure.\n- [ ] Contract state and details are accessible through the modified contracts-ui by querying contract address and viewing basic information (address, state, calls).\n- [ ] Performance comparison tests show measurable improvements over ink! and pallet-contracts implementations.\n\n## Related Work\n\n- [Polkadot SDK forkless runtime upgrade mechanism](https://docs.polkadot.com/develop/parachains/maintenance/runtime-upgrades/).\n- Existing smart contract engines:\n    - [EVM Documentation](https://ethereum.org/en/developers/docs/evm/) and [Ethereum yellowpaper](https://ethereum.github.io/yellowpaper/paper.pdf);\n    - [Wasm within the Polkadot SDK](https://docs.polkadot.com/develop/smart-contracts/wasm-ink/);\n    - [Solana BPF](https://solana.com/docs/core/programs).\n- [RISC-V Specification](https://riscv.org/specifications/ratified/).\n- [Ethereum ABI Specification](https://docs.soliditylang.org/en/develop/abi-spec.html).\n- Smart Contract Performance and Analysis Research:\n    - [Latency-First Smart Contract Design: Using speculative execution to reduce transaction latency](https://eprint.iacr.org/2023/951.pdf).\n    - [Evaluating Smart Contract Performance in a Production-like Environment](https://dlt2024.di.unito.it/wp-content/uploads/2024/05/DLT2024_paper_62.pdf).\n    - [Evaluating Solidity Smart Contract Code: Metrics for Complexity and Security](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0281043).\n- Formal Verification Tools:\n    - [K Framework](https://kframework.org/);\n    - Proof assistant tools [Coq Proof Assistant](https://coq.inria.fr/) and [Isabelle](https://isabelle.in.tum.de/);\n    - [A curated list of awesome formal verification resources for Web3](https://github.com/johnsonstephan/awesome-web3-formal-verification).\n- [Fuzzing in the Polkadot](https://security.parity.io/fuzzing).\n- Projects exploring RISC-V instruction set for blockchain:\n    - [CKB-VM by Nervos](https://docs.nervos.org/docs/tech-explanation/ckb-vm);\n    - [RISC Zero is zk verifiable general computing platform based on zk-STARKs and the RISC-V](https://github.com/risc0/risc0);\n    - [Ethereum Execution Environment that integrates RISC-V smart contracts](https://github.com/r55-eth/r55);\n    - [RISC-V instruction set extension on blockchain application](http://eprints.utar.edu.my/6486/1/fyp_CT_2024_CKS.pdf).\n- [Polkadot.js Apps](https://polkadot.js.org/apps/) and [QF Block explorer](https://dev.qfnetwork.xyz/#/explorer) interface.\n- [Substrate Contract Development](https://docs.polkadot.com/develop/smart-contracts/).\n- PolkaVM [GitHub repository](https://github.com/paritytech/polkavm).\n- [Transactions fees and gas metering implementation in Polkadot SDK](https://docs.polkadot.com/polkadot-protocol/basics/blocks-transactions-fees/fees/).\n- [QF implementation of PolkaVM Blob Hashing and Addressing](https://github.com/QuantumFusion-network/spec/blob/main/docs/PolkaVM/blob_hashing_addressing.md).\n- Storage rent implementations:\n    - [Solana's account rent system](https://docs.solana.com/learn/rent)\n    - [Near Protocol's storage staking](https://docs.near.org/concepts/storage/storage-staking)\n- [contracts-ui Repository](https://github.com/use-ink/contracts-ui) - UI for interacting with smart contracts.\n- [Create Polkadot dApp GitHub repo by Paritytech](https://github.com/paritytech/create-polkadot-dapp).\n\n## Constraints\n\n- Integration must maintain backward compatibility with existing QF runtimes and Smart Contracts v0 implementation.\n- Emulation for EVM introduces performance trade-offs; optimization is critical.\n- EVM compatibility must cover most common Solidity patterns but doesn't need to support every edge case.\n- Advanced functions (e.g., ZKP verification) require resource-efficient implementations to avoid slowing execution.\n- Performance optimizations must not reduce security guarantees.\n- Gas limits must be flexible but prevent block time degradation.\n- Formal verification tools must be usable by average developers without specialized knowledge.\n- Contract-to-contract calls must maintain proper isolation and security boundaries.\n\n## Risks\n\n- EVM compatibility emulation may underperform compared to native EVM environments.\n  - Mitigation: Focus on optimizing the most common operations first and clearly document any performance trade-offs.\n\n- Contract-to-contract call implementation could introduce security vulnerabilities.\n  - Mitigation: Implement comprehensive test suite focusing on isolation, input sanitization, and error handling; conduct formal security audit.\n\n- Solidity projects may encounter compatibility or re-certification challenges during porting.\n  - Mitigation: Provide detailed migration guides and validation tools to verify functional equivalence after migration.\n\n- EVM compatibility layer could have unexpected behavior with complex contracts.\n  - Mitigation: Begin with support for well-established standards (ERC tokens) and gradually expand to more complex patterns; document known limitations.\n\n- RISC-V implementation and integration into QF may require significant development effort.\n  - Mitigation: Prioritize core functionality first, establish clear milestones, and consider parallel development tracks.\n\n- Precompiled functions might become bottlenecks under heavy load.\n  - Mitigation: Implement resource throttling and benchmark under various load conditions to identify and address potential bottlenecks.\n\n- Performance optimization might introduce new security vulnerabilities.\n  - Mitigation: Include security review as part of the optimization cycle and prioritize maintaining security over marginal performance gains.\n\n- Storage rent system could create unexpected economic incentives.\n  - Mitigation: Simulate various economic scenarios, start with conservative pricing, and implement mechanisms to adjust parameters through governance.\n\n- Smart contract deployed through PolkaVM might expose vulnerabilities if the deployment process is not correctly validated.\n  - Mitigation: Implement comprehensive validation checks during deployment and provide tools to analyze contracts before deployment.\n\n- Performance benchmarks might not reflect real-world usage.\n  - Mitigation: Design benchmarks based on actual dApp patterns and include realistic mixed workloads rather than isolated function tests.\n\n- Adapted contracts-ui might not support all required features for QF smart contracts.\n  - Mitigation: Prioritize essential features and develop custom components as needed.\n\n## Development Outcomes\n\n- Optimized PolkaVM implementation with documented performance improvements and benchmark results compared to EVM and Wasm.\n- Working contract-to-contract call mechanism with comprehensive security measures and documentation.\n- Working EVM compatibility layer with verified support for common Solidity patterns and ERC token standards.\n- Gas limits research results with optimal values for different contract types under various network conditions.\n- Compiler toolchain supporting multiple programming languages with examples and language-specific best practices.\n- Advanced precompiles implementation (Merkle proofs, crypto operations) with performance metrics and usage documentation.\n- Storage rent system with economic analysis and tuning recommendations.\n- Developer tools suite including:\n  - Enhanced debugger with source mapping\n  - Resource monitoring dashboard\n  - Local testing environment\n  - Formal verification integration\n- Extended documentation covering:\n  - Language support and best practices\n  - Optimization techniques\n  - Security considerations\n  - EVM compatibility details\n  - Contract-to-contract interaction patterns\n  - Storage system economic model\n- Smart contract event system implementation with indexing, filtering, and subscription capabilities for dApp integrations.\n- Performance analysis toolkit with comparison reports and resource usage statistics.\n- Migrated dApp examples from pvm-dapp-demo repository with updated interaction methods.\n- Comprehensive dApp development guide with step-by-step tutorials.\n- Adapted contracts-ui implementation with QF-specific features and styling.\n- Stability and performance test suite with comparison metrics against other platforms.\n\n## Decision Log\n\n- Decision 1: Adopt RISC-V instruction set for PolkaVM implementation.\n- Decision 2: Include an EVM compatibility layer to facilitate Solidity-based project migration.\n- Decision 3: Prioritize benchmarks and performance analysis as part of acceptance criteria.\n- Decision 4: Implementation will build upon the Smart Contracts v0 foundation established in the TestNet case.\n\n## Notes\n\n- This case specifically builds on the foundation of \"Smart contract environment for TestNet\" and assumes its successful implementation.\n- PolkaVM offers significant innovation potential, allowing the execution of diverse programming environments as smart contracts.\n- Successful implementation may position QF as a leading platform for high-performance, permissionless smart contract execution.\n- Future enhancements may explore further compiler integrations and optimization of advanced precompile functions.\n\n## Learnings\n- We use `hash(blob_owner, blob_version)` as an address in the first implementation of smart cantracts addressing (https://github.com/QuantumFusion-network/spec/issues/69) \n- XCQ & PVM: https://github.com/QuantumFusion-network/spec/issues/114",
                "url": "https://github.com/QuantumFusion-network/spec/issues/145",
                "comments": []
            },
            {
                "number": 113,
                "title": "Infrastructure and Development processes",
                "state": "open",
                "author": "actinfer",
                "labels": [
                    "In Use"
                ],
                "created_at": "2025-01-27T07:13:06Z",
                "updated_at": "2025-06-03T10:47:45Z",
                "closed_at": null,
                "body": "This case focuses on establishing the necessary infrastructure and making key development process decisions to optimize and streamline engineering workflows. The goal is to enhance team efficiency and support scalable development practices by addressing gaps in infrastructure, automation, and collaboration tools and guidelines.\n\nThe scope includes identifying and improving workflows related to CI/CD pipelines, infrastructure automation, and collaborative engineering processes. It aims to equip teams with tools and best practices that simplify daily operations while aligning with organizational objectives.\n\n## Objectives\n\n- Research best practices for blockchain network infrastructure and operations.\n- Set up and document standardized infrastructure for QF blockchain nodes (validators, RPC, archive) with proper security configurations.\n- Establish CI/CD pipelines for automating code quality checks and deployments of the blockchain node repository.\n- Implement Infrastructure-as-Code (IaC) practices to automate configuration and management.\n- Configure comprehensive monitoring and alerting for infrastructure and blockchain nodes.\n- Ensure infrastructure and blockchain nodes resiliency and aligned security.\n- Introduce and enhance collaboration tools and automation frameworks to improve developer workflows.\n- Develop process guides to facilitate developer collaboration.\n- Deploy high-availability solutions for critical network services through load balancing.\n- Set up secure access controls and firewall configurations for infrastructure protection.\n\n## Acceptance Criteria\n\n- [x] Basic development processes and secure private key management is documented.\n- [x] CI workflow is set up with automated code quality and build success checks for blockchain node repository PRs.\n- [x] Infrastructure inventory documented, initial configuration automated.\n- [x] Infrastructure uptime monitoring and alerts configured (loki, prometric).\n- [x] Blockchain nodes uptime and block production monitoring configured with alerts, logging system parses and displays blockchain nodes and services logs in Grafana.\n- [x] Alerting system sends alerts to Zulip chat and an dev team emails.\n- [x] Grafana dashboard with block time plot and block production success indicator.\n- [x] Indexer service database reservation is implemented for TestNet (qf-squid, PostgreSQL cluster as an indexer database, Hasura GQL engine).\n- [ ] Security configuration with ACL/firewall setup is implemented for infrastructure nodes and RPC endpoints if needed.\n- [ ] DNS load balancing (Cloudflare) is configured for high availability of RPC endpoints, validators cluster, bootnodes, and service APIs.\n- [x] Base infrastructure cluster is operational with machines, blockchain nodes, and services described in [spec/docs/TestNet](https://github.com/QuantumFusion-network/spec/tree/main/docs/TestNet_infra).\n\n## Related Work\n\n- Previous efforts to improve CI/CD pipelines.\n- Existing automation tools and scripts.\n- Documentation on team workflows and dependencies.\n- [Parachain DevOps Guide](https://paritytech.github.io/devops-guide/overview.html).\n- [Polkadot Secure Validator Setup](https://wiki.polkadot.network/docs/maintain-guides-secure-validator).\n- [Polkadot Node Monitoring](https://wiki.polkadot.network/docs/maintain-guides-how-to-monitor-your-node).\n\n## Constraints\n\n- Limited DevOps capacity available for implementation and ongoing maintenance.\n- Infrastructure solutions must work across multiple cloud providers and bare metal environments.\n\n## Risks\n\n- Risk: Unreliable monitoring and alerting, monitoring and alerting may not provide meaningful insights, fail to detect failures, or miss notifications.\n  Mitigation: follow best practices for blockchain network monitoring and choose an observability SaaS over a self-hosted service.\n\n- Risk: Poor security configuration, secrets may be publicly visible, or APIs may be insecurely exposed.\n  Mitigation: define secrets and private keys management guide, use a widely adopted SaaS for secrets management, follow blockchain node RPC security best practices.\n\n- Risk: Insufficient DevOps resources for network maintenance, a lack of attention to maintenance may lead to an insecure, unstable, or faulty network.\n  Mitigation: plan for DevOps-related roles and capacity in advance.\n\n- Risk: Limited expertise in blockchain validator management, a lack of knowledge in validator management can result in network instability and faults.\n  Mitigation: allocate resources for learning validator management best practices and guides.\n\n- Risk: Single points of failure in infrastructure causing network downtime.\n  Mitigation: Deploy redundant nodes across different geographical locations and implement automated failover mechanisms.\n\n- Risk: Automation failures causing unexpected behaviour during deployments.\n  Mitigation: Implement progressive deployment strategies with automated testing and rollback capabilities.\n\n- Risk: Network performance degradation under load.\n  Mitigation: Conduct load testing and establish performance baselines with automated scaling capabilities.\n\n## Development Outcomes\n\n- Operational infrastructure environment with documented deployment procedures.\n- Automated CI/CD pipeline integrated with GitHub for blockchain node repository.\n- Infrastructure-as-Code repositories containing configurations for reproducible deployments.\n- Monitoring dashboards showing real-time blockchain performance metrics.\n- Documentation containing:\n  - Secure key management procedures;\n  - Node operation guides;\n  - Infrastructure scaling guidelines.\n- Security documentation covering:\n  - Access control policies;\n  - Firewall configuration;\n  - Vulnerability management process.\n- Load balancing configuration for high-availability services.\n\n## Decision Log\n- Database backups not required for DevNet due to its temporary nature. 2025.03.07\n\n## Notes\n\n[Any additional information or context]",
                "url": "https://github.com/QuantumFusion-network/spec/issues/113",
                "comments": [
                    {
                        "author": "dedok",
                        "created_at": "2025-05-07T06:53:30Z",
                        "body": "We are having an issue with creating email group. I changed criteria according this reality."
                    }
                ]
            },
            {
                "number": 34,
                "title": "QF Ecosystem Portal, Management and Development Tools",
                "state": "open",
                "author": "kirushik",
                "labels": [
                    "In Use"
                ],
                "created_at": "2024-12-19T11:35:51Z",
                "updated_at": "2025-05-06T07:44:01Z",
                "closed_at": null,
                "body": "This case focuses on developing and deploying a comprehensive ecosystem portal and related tools for the Quantum Fusion Network. This includes a feature-rich blockchain portal, tools for dApp developers, infrastructure tools to help with node management and integration with exchanges, and built-in wallet functionalities and a telemetry overview. \n\n## Objectives\n-   Implement a feature-rich blockchain portal to display comprehensive QF data (blocks, transactions, accounts, and smart contract details) and to provide insights into the network activity.\n    - Implement a real-time transaction monitoring dashboard in the portal UI, showing transaction status and confirmations.\n    - Integrate a basic network status overview (e.g., node count, block production rate) into the portal.\n    - Implement a method to display PolkaVM contract functions and storage directly in the portal UI.\n    - Ensure the portal UI uses the provided design and branding for QF.\n    - Integrate a basic wallet functionality into the portal.\n    - Display telemetry data directly into the portal.\n    - (Optional) Implement an Account Format Transformation tool.\n    - (Optional) Implement a simple Price Converter.\n-   Implement dApp developer tools for testing and debugging QF features, including smart contracts.\n-   Implement a Metamask \"snap\" for user-friendly QF interaction, using a familiar interface for users.\n-   Implement a separate browser extension and mobile-based wallets supporting full featureset of QF and its RPC.\n-   Implement Helm charts for streamlined Kubernetes node deployments, including telemetry.\n-   Implement a sidecar service for seamless QF token integration with exchanges.\n-   Create comprehensive technical documentation for setup, maintenance, and usage, including user guides, API references, and practical examples for all the tools and components.\n\n## Alignment with Other Cases\n\n- **DevNet:** Tools will leverage the DevNet for stable deployments and runtime upgrades.\n- **RPC:** Tools will utilize the Cap’n Proto RPC for efficient and reliable communication.\n- **PolkaVM:** Tools will support PolkaVM for smart contract interactions.\n- **RPC Layer:** Tools will be aligned with the Cap'n Proto RPC layer for a Web2-like experience.\n\n## Acceptance Criteria\n\n- [ ] The blockchain portal accurately displays all relevant QF data (blocks, transactions, accounts, and smart contract details).\n    - [ ] A real-time transaction monitoring dashboard is implemented in the UI, showing transaction status and confirmations.\n    - [ ] A basic network status overview (e.g., node count, block production rate) is visible within the portal.\n    - [ ] The portal correctly displays PolkaVM user-uploaded smart contract features and storage information.\n    - [ ] A basic wallet functionality is implemented in the portal, allowing users to view their balances and transfer tokens.\n    - [ ] Telemetry data is correctly displayed in the portal, providing an overview of node and network health.\n    - [ ] (Optional) An Account Format Transformation tool can transform a QF address into a valid address.\n    - [ ] (Optional) A simple Price Converter is implemented and shows a price corresponding to market rates.\n    - [ ] The portal UI uses the provided design and branding for QF.\n- [ ] Developer tools enable seamless dApp debugging and feature testing, including smart contracts.\n- [ ] Users can connect the Metamask to the QF network and perform a basic operation, such as retrieving the balance, through a familiar interface.\n- [ ] Show that there's a separate extension in Chrome Web Store, which allows creating, importing and controlling QF account.\n- [ ] There's an app in Android Play Store, which when installed allows user to create, import and control a QF account.\n- [ ] Helm charts can be used to deploy a node to a test Kubernetes cluster.\n- [ ] An exchange sidecar API can be used to simulate an exchange integration workflow and test a deposit or withdrawal of QF tokens.\n- [ ] All tools and the portal are well-documented, intuitive, and performant.\n\n## Related Work\n\n  - Block explorers and portals: [Subscan](https://github.com/subscan-explorer), [Polkascan](https://github.com/polkascan).\n  - [Polkadot.js](https://polkadot.js.org).\n  - [Helm charts by Parity](https://github.com/paritytech/helm-charts).\n  - Exchange integration: [Substrate API sidecar](https://github.com/paritytech/substrate-api-sidecar).\n  - [Polkadot Wallet Snap for MetaMask](https://github.com/ChainSafe/metamask-snap-polkadot).\n  - Wallets: \n    - [official list of Polkadot compatible wallets](https://polkadot.com/get-started/wallets),\n    - GitHub repos: [Nova wallet](https://github.com/novasamatech/nova-wallet-android) Andriod app, [Polkadot.js](https://github.com/polkadot-js/extension),[SubWallet](https://github.com/Koniverse/SubWallet-Extension) and [Talisman](https://github.com/talismansociety/talisman) browser extensions,\n    - [Metamask Snaps](https://docs.metamask.io/snaps/), \n    - [Trustwallet Documentation](https://developer.trustwallet.com/developer), \n    - [Ledger Documentation](https://developers.ledger.com/).\n\n## Constraints\n-  Must be compatible with QF runtime.\n-  Must work correctly with the QF public RPC.\n-  Must work correctly when deployed on Kubernetes.\n-  Must support cross-chain communication with Polkadot using XCM.\n\n## Risks\n\n-  There might be difficulties connecting tools with the QF runtime and other ecosystem components, or with the RPC server or with the Polkadot .\n-  The portal and the tools might not show information quickly enough if the network is heavily used or when retrieving PolkaVM state.\n-  The portal might not be user-friendly enough or have problems with responsiveness and accessibility.\n- The implemented debugging tools might not be able to catch subtle but important errors in the dApps, which may lead to unexpected behavior or security flaws.\n- The Helm charts might not work as expected with the Kubernetes environment, or there might be issues related to version compatibility, or missconfiguration causing deployment failures.\n-  If not implemented carefully, there might be some XSS vulnerabilities, or the data might not be properly sanitized when displayed. Also, tools may have security flaws if they are not correctly implemented.\n-  Improper wallet implementation might expose private keys.\n-  Telemetry implementation might be unstable or not work as intended.\n\n## Development Outcomes\n\n-  A working and deployed blockchain portal that accurately displays QF data, is well-documented, tested, and branded using provided assets and guidelines.\n    -  A real-time transaction dashboard that is easy to use and provides clear visual feedback about the current status of the transactions.\n    -  An implementation that allows users to view basic information of the QF network such as the number of nodes or the block production rate.\n    -  A section within the block explorer that shows PolkaVM contract functions and its storage.\n    -  A basic wallet functionality is implemented within the portal, allowing users to view their balances and send tokens.\n    -  The portal displays a basic set of telemetry data.\n    -  (Optional) A working Account Format Transformation tool.\n    -  (Optional) A working Price Converter.\n-  A working set of dApp developer tools for seamless development, including testing of the PolkaVM contracts.\n-  A functional Metamask snap to provide user-friendly QF interaction.\n-  A working browser extension in the Chrome Web Store that allows creating, importing and controlling a QF account.\n-  A working app in Android Play Store that allows users to create, import and control a QF account.\n-  A working deployment system for QF nodes with Helm charts, telemetry, and Kubernetes.\n-  A sidecar service deployed for exchanges to integrate QF tokens easily.\n-  Complete technical documentation with user guides, API references and examples for all implemented tools and components, including the portal and all its functionality.\n\n## Decision Log\n[Record key decisions made during the case]\n\n## Notes\n\n**Additional Tools for Future Implementation:**\n  - GraphQL API Gateway\n  - Decentralized Identity (DID) Framework\n  - Multi-Signature Wallet Management Tools\n  - Governance Dashboard\n  - Smart Contract Verification Tool\n  - NFT Management Suite\n  - Off-Chain Data Oracle Integration Framework\n  - Testing and Simulation Environments\n  - Cross-Chain Bridge Dashboard\n  - Monitoring and Alerting System\n  - Energy Consumption Analytics Tool\n\n**Additional Tasks:**\n  - Tool localization\n  - Accessibility of tools",
                "url": "https://github.com/QuantumFusion-network/spec/issues/34",
                "comments": []
            },
            {
                "number": 31,
                "title": "Offchain storage",
                "state": "open",
                "author": "kirushik",
                "labels": [
                    "Accepted"
                ],
                "created_at": "2024-12-19T11:20:30Z",
                "updated_at": "2025-02-13T12:08:51Z",
                "closed_at": null,
                "body": "Develop an offchain storage solution using IROH to provide efficient, distributed, and crypto-verifiable data storage capabilities for QF dApps while maintaining seamless integration with the blockchain's core functionality.\n\n## Objectives\n\n- Implement IROH-based distributed storage system with content-addressable data model for QF dApps.\n- Develop a verifiable link between onchain references and offchain IROH content.\n- Design and implement storage incentivization mechanisms for network participants.\n\n\n## Acceptance Criteria\n\n- IROH storage system is fully operational and integrated with QF SDK.\n- Crypto-verification system proves data integrity between chain and IROH storage. \n- Storage incentivization mechanism is implemented and tested.\n- Performance benchmarks demonstrate scalability up to 10TB of network storage.\n\n## Related Work\n\n- IROH distributed storage system (https://www.iroh.computer/docs/)\n- IPFS peer-to-peer content delivery network (https://ipfs.tech/)\n- Filecoin decentralized storage network (https://filecoin.io/)\n- BitTorrent protocol specification (https://www.bittorrent.org/beps/bep_0003.html)\n\nCompetitors:\n\n- Arweave permanent information storage (https://www.arweave.org/)\n- Swarm decentralized storage (https://www.ethswarm.org/)\n- Ceramic Network (https://ceramic.network/)\n\n## Constraints\n\n- Must maintain compatibility with QF's networking layer.\n- Storage providers must be able to prove data availability.\n\n## Risks\n\n- Complex integration between blockchain and IROH storage layers could introduce vulnerabilities.\n- Storage incentivization mechanism could be gamed or manipulated.\n- Network congestion during peak usage could affect retrieval times.\n\n## Learning Outcomes \n\n- Technical analysis of IROH implementation with QF blockchain.\n- Storage provider incentivization model effectiveness report\n- Performance benchmarks under various network conditions and data sizes.\n\n## Decision Log\n\n- Decision 1: Use IROH as primary storage layer\n- Decision 2: Design merkle-based verification system linking chain and storage\n- Decision 3: Design incentivization model for storage providers\n\n## Notes\n\n- Need to create efficient chunking strategy for large files\n- Consider to implement garbage collection for outdated content\n- Documentation with practical examples should be developed ",
                "url": "https://github.com/QuantumFusion-network/spec/issues/31",
                "comments": []
            },
            {
                "number": 30,
                "title": "DevNet launch",
                "state": "closed",
                "author": "actinfer",
                "labels": [
                    "In Production"
                ],
                "created_at": "2024-12-18T14:10:00Z",
                "updated_at": "2025-04-01T09:05:09Z",
                "closed_at": "2025-03-05T12:28:21Z",
                "body": "Сreating a DevNet environment to integrate and showcase PolkaVM functionality. \n\nWe promise to the community that DevNet will be launched by the end of the year 2024. This will be our first major community milestone. It will show that the main technology works and let our community test it.\n\nDevNet will include three validator nodes hosted in a single data center that will reliably communicate, produce blocks, maintain consensus, and run PolkaVM code. The network will be publicly accessible via RPC, and will allow anyone to connect their own (non-validating) nodes.\n\nThis DevNet is an important first step. It lets us test the main functions of our blockchain. This isolated environment lets us try new things safely without any impact on the MainNet. It provides a safe and controlled environment to test and adjust the basic blockchain functions before releasing more complex features.\n\n## Objectives\n- Deploy 3 DevNet validator nodes on a bare Ubuntu server using `systemd` scripts.\n- Set up 3 DevNet validator nodes that can connect, communicate, and produce blocks.\n- Set up basic telemetry to track node and network health in real time and display it publicly (e.g. [telemetry.polkadot.io](https://telemetry.polkadot.io/)).\n- Test block production and simple smart contract execution using PolkaVM on a DevNet.\n- Set up a publicly available RPC server to interact with the DevNet.\n- Ensure the DevNet allows connections from external (non-validating) nodes.\n- Implement a faucet that distributes small amounts of tokens to users who request them.\n- Deploy a block viewer to display real-time blockchain data (e.g. [Polkadot Portal](https://polkadot.js.org/apps/#/explorer)).\n- Test runtime upgrades and Substrate-specific features under realistic network conditions.\n- Establish foundations for smart contract execution using PolkaVM.\n\n## Secondary Objective\n- Improve development and collaboration processes on GitHub.\n\n## Acceptance Criteria\n- [x] 3 DevNet validator nodes connect, communicate with each other, and produce blocks.\n- [x] There's a publicly available telemetry service observing QF DevNet nodes.\n- [x] Additional full nodes can be provisioned automatically on an empty Ubuntu server, using a straightforward automated process.\n- [x] It is possible to transfer balances between accounts.\n- [x] Only QF core team has root-level access to the DevNet runtime.\n- [x] There is a publicly-usable faucet (or other source of funds) dispersing DevNet tokens to the requested accounts.\n- [x] There is a publicly-reachable RPC server hosted by QF.\n- [x] Non-validating node can be connected to the DevNet.\n- [x] There is a smart contract deployed, and a simple dApp is online publicly for interacting with it.\n- [x] There's a block indexer running, with a publicly available GUI allowing querying it for the up-to-date on-chain data.\n- [x] Public block indexing GUI uses design and branding from QF.\n\n## Related Work\n- [Official Polkadot documentation](https://wiki.polkadot.network)\n- [Current Polkadot monorepository](https://github.com/paritytech/polkadot-sdk)\n- https://contracts.polkadot.io/\n- [Polkadot validator hardware requirements](https://wiki.polkadot.network/docs/maintain-guides-how-to-validate-polkadot).\n- [Polkadot Telemetry project](https://github.com/paritytech/substrate-telemetry)\n- Competitors:\n  - Ethereum DevNet [documentation](https://ethereum.org/en/developers/docs/development-networks/) and [Kurtosis package](https://github.com/ethpandaops/ethereum-package)\n  - [Avalanche DevNet documentation](https://docs.avax.network/tooling/create-avalanche-nodes/setup-devnet)\n\n## Constraints\n- Initial node setup will be manual (using `systemd`).\n- Limited DevOps support during the initial phase.\n\n## Risks\n- Our modifications of Substrate codebase turn out to be unstable in production.\n- Issues in telemetry integration may prevent us from collecting the metrics we need or be too unreliable.\n- Block production stalls due to the misconfiguration of validator nodes.\n- The block viewer may display incorrect or incomplete blockchain data. \n- Public RPC server stability issues cause delays in the block viewer.\n- Lack of mature tooling around PolkaVM prevents reliable smart contract deployment or execution.\n\n## Learning Outcomes\n- Understanding subtleties of running Substrate-based nodes in production environment.\n- Hardware requirements of running Validator and RPC nodes for QF DevNet needs.\n- DevOps knowledge and automation useful for provisioning QF nodes.\n\n## Decision Log\n1. `systemd` will be used to manage nodes as they are deployed and used - Vas Soshnikov 241217 14-23.\n\n## Notes\n\n### Preparatory Steps for PolkaVM:\n- Develop a test pallet to execute a simple addition function (incrementing account balance) in PolkaVM runtime without parameters. The result should be an operation recorded in a block.\n- Create an extrinsic to ensure test transactions both reflect in the database and generate a new block. Address the current limitation where PolkaVM updates balances but does not trigger block production.\n- Build a microservice with two endpoints: one to upload a blob and another to execute it.\n\n### Requirements for DevNet Launch:\n- Procure and set up VMs with static IP addresses.\n- Link IPs via QF domain with SSL.\n- Load SSH keys for developers to access nodes.\n- Deploy and link three nodes manually (without automation, with systemd).\n- Deploy a block viewer (e.g., polkadot.js-based).\n\n### Optionally:\n- Allow user-uploaded smart contracts via Polkadot.js UI.\n- Provide a README.md for QF node setup.\n\n### Future Steps:\n- Hire DevOps personnel.\n- Automate node deployment using GitHub Actions.\n- Implement monitoring systems with DevOps assistance.",
                "url": "https://github.com/QuantumFusion-network/spec/issues/30",
                "comments": [
                    {
                        "author": "dedok",
                        "created_at": "2025-02-06T08:18:52Z",
                        "body": "All access & url stored in bitwarden, if you need access please read this: https://github.com/QuantumFusion-network/infra/blob/main/docs/key_management_basic.md"
                    },
                    {
                        "author": "actinfer",
                        "created_at": "2025-03-05T12:28:07Z",
                        "body": "- Case acceptance criteria were checked and accepted by stakeholders\n- Experiment results reviewed agreed on with @khssnv \n- Case closed"
                    }
                ]
            },
            {
                "number": 27,
                "title": "Non-web SDK",
                "state": "open",
                "author": "actinfer",
                "labels": [
                    "Accepted"
                ],
                "created_at": "2024-12-15T07:36:41Z",
                "updated_at": "2025-02-13T12:08:33Z",
                "closed_at": null,
                "body": "Develop a suite of native (SDKs) for non-web environments to extend QF's blockchain ecosystem beyond the browser. By providing native SDKs for major platforms such as iOS, Android, and Windows, QF aims to empower developers to build decentralized applications on mobile, desktop, and embedded systems with the ease and familiarity of traditional development tools. \n\n### Objectives\n\n- Develop Native SDKs: Create fully functional and optimized SDKs for major platforms including iOS (Swift), Android (Kotlin/Java), and Windows (C++/C#).\n- Ensure SDK initialization and core functions execute under 200ms on standard devices for 95% of operations. \n- Provide comprehensive documentation covering 100% of SDK features.\n\n### Acceptance Criteria\n\n- SDK is developed and accessible in GitHub for target platforms: iOS, Android, and Windows.\n- SDKs pass all unit and integration tests on target platforms. \n- SDK initialization and core functions execute under 200ms on standard devices for 95% of operations.\n- Documentation is covering 100% of SDK features and includes practical examples and tutorials.\n\n### Related Work\n\n- Polkadot Java library (https://github.com/polkadot-java/api)\n- Kotlin programming language (https://kotlinlang.org/)\n- Swift programming language (https://www.swift.org/)\n- C++ programming language (https://isocpp.org/)\n\nCompetitors: \n\n- Web3j Android library for Ethereum (https://www.web3labs.com/web3j-sdk)\n- Solana.Swift (https://github.com/metaplex-foundation/Solana.Swift)\n- Aleth Ethereum C++ SDK (https://github.com/ethereum/aleth)\n\n### Constraints\n\n- Must be fully compatible with QF's blockchain protocols.\n- Resource limitations on certain devices require efficient memory and CPU usage.\n\n### Risks\n\n- Performance issues on lower-end devices.\n- High maintenance effort across platforms.\n- Complexity may slow development velocity.\n\n### Learning Outcomes \n\n- Gained experience in cross-platform SDK development.\n- Improved multi-platform release management.\n- Understanding developer needs on different platforms.\n\n[To be expanded during implementation]\n\n### Decision Log\n\n- Decision 1: Research market demand, developer needs and expectations\n- Decision 2: Utilize native languages and frameworks for each platform to ensure optimal performance and developer familiarity (Java, C++, Kotlin, Swift) \n\n### Notes\n\n- Gather feedback from native blockchain SDK users\n- Engage community for input on design, priorities\n- Consider partnership opportunities for niche use cases\n- Set up analytics to track SDK usage and gather feedback.\n",
                "url": "https://github.com/QuantumFusion-network/spec/issues/27",
                "comments": []
            },
            {
                "number": 17,
                "title": "Light clients",
                "state": "open",
                "author": "actinfer",
                "labels": [
                    "Accepted"
                ],
                "created_at": "2024-12-15T06:15:17Z",
                "updated_at": "2025-02-13T12:10:06Z",
                "closed_at": null,
                "body": "This case focuses on the design, development, and optimization of light client functionality for the Quantum Fusion Blockchain Network. The primary goal is to enable broad access to QF dApps via mobile and low-bandwidth devices, reducing reliance on centralized RPC providers. This case is significant as it directly impacts the usability and accessibility of the QF network by enabling users with limited resources to participate on the chain, without needing to rely on RPC providers or full nodes.\n\n## Objectives\n- Implement a robust and efficient light client (e.g. based on the Smoldot library) that can operate on low-resource devices, including mobile.\n- Optimize the extraction and provision of Merkle proofs to minimize the load on full nodes (1,000 light nodes per full node) and ensure fast response times for light clients (1 sec per RPC request), while also reducing the size of required data.\n- Research and implement a suitable storage solution for light clients that defines which blockchain data should be stored locally versus retrieved via Merkle proofs, considering the trade-offs between data size, and access speed, and explore the best approaches to store light client state. Document the chosen storage solution and the rationale behind its selection for each target platform (web browsers, mobile apps).\n- Ensure seamless integration of the light client with the QF SDK across multiple platforms (web browsers, mobile apps) using WebRTC for browser based light clients.\n- Light clients should support peer-to-peer connections to both full nodes and other light clients, to exchange of transactions and blocks meta-information, and be able to submit transactions through the network.\n- (Optional) Explore the feasibility of light clients on embedded systems.\n\n## Acceptance Criteria\n- [ ] Light client core functionality and resource efficiency maintains stable operation during 30-minute benchmark on low-resource device (2GB RAM, single core CPU).\n- [ ] Light client performance on mobile devices maintains sub-second response times and efficient resource usage by benchmarking on smartphone with 5G connection performing various actions (e.g., viewing account balance, submitting transactions) for 10 minutes. This benchmark must include monitoring of memory and CPU usage.\n- [ ] Merkle proof generation and verification completes within 1 second while monitoring full node handling 1,000 concurrent light client connections through 10-minute benchmark.\n- [ ] Light client storage strategy clearly defines data storage and retrieval patterns by documenting chosen solution with benchmarks for local storage usage and Merkle proof retrieval times across different data types.\n- [ ] Light client storage implementation meets size and speed requirements by running 10-minute platform-specific benchmark on browser and mobile platforms with continuous data access patterns.\n- [ ] QF SDK and WebRTC integration functions correctly across platforms by benchmarking main SDK operations through WebRTC connections in Chrome (optional: Firefox and Safari) for 5 minutes and ensuring all operations complete successfully without errors.\n- [ ] Light client P2P functionality successfully maintains connections and exchanges data by executing 5-minute network benchmark of peer connections, block data exchange, and transaction processing.\n- [ ] Light client synchronization meets time requirements by measuring initial sync under 1 minute and updates under 5 seconds. \n- [ ] Light client should be able to verify the finalization signatures of block headers.\n- [ ] The light client implementation will be documented and clear to understand by developers.\n- [ ] (Optional) Light client embedded system compatibility meets performance requirements through 30-minute operation benchmark on target hardware within 85% of documented resource limits.\n\n## Related Work\n- [Smoldot Github repository](https://github.com/smol-dot/smoldot): Official repository for Smoldot library.\n- [Merkle proofs basic explanation](https://decrypt.co/resources/merkle-trees-guide-explainer-blockchain).\n- [WebRTC samples collection](https://webrtc.github.io/samples/) and [guide to peer connections](https://webrtc.org/getting-started/peer-connections).\n\n## Constraints\n- Limited storage capacity and network bandwidth on mobile devices.\n- Need to balance between performance and security of light clients.\n- Ensuring that light clients are not dependent on centralized infrastructure and can use peer-to-peer connections.\n- Light client implementation must be compatible with WebRTC.\n- The implementation must minimize the amount of data that needs to be stored and transfered.\n\n## Risks\n- Performance bottlenecks in the generation of Merkle proofs or state retrieval could degrade light client performance, or slow down node performance.\n- Security vulnerabilities in the light client implementation could compromise user data, or network integrity by misinterpreting data.\n- Incompatibility issues between different platforms (web browsers, native mobile) and their integration with the QF SDK, or issues with WebRTC and the limitations of its usage in the browser.\n- Storage limitations on mobile devices may affect the ability of the client to function correctly.\n- The light client may have difficulties synchronizing data with the chain due to various implementation errors or networking issues.\n\n## Development Outcomes\n- A working and deployable light client implementation.\n- An optimized way to generate Merkle proofs that is fast enough for light clients.\n- A well defined process on what data must be stored by light clients, and what data can be requested from the network.\n- A set of clear API and well documented SDK that can be used by dApp developers to utilize the QF light clients.\n- Clear integration guidelines for different platforms (web browsers, native mobile) when implementing SDK, specifically for usage with WebRTC.\n- A working light client that can successfully connect to the network, and perform basic operations.\n- Clear and concise documentation for setup and troubleshooting the light client.",
                "url": "https://github.com/QuantumFusion-network/spec/issues/17",
                "comments": []
            },
            {
                "number": 15,
                "title": "Internal optimizations",
                "state": "open",
                "author": "actinfer",
                "labels": [
                    "Accepted"
                ],
                "created_at": "2024-12-15T05:59:00Z",
                "updated_at": "2025-02-13T12:09:46Z",
                "closed_at": null,
                "body": "This case focuses on identifying, implementing, and evaluating internal optimizations for the Quantum Fusion Network blockchain. The goal is to achieve a significant (x2-x3) performance improvement over a standard Substrate implementation by targeting frequently executed code paths, reducing overhead, and improving the overall debuggability of the system. A key aspect of this work is to analyze if the changes require a Substrate fork or if they can be applied directly to the current code.\n\n## Objectives\n- Optimize Substrate in the context of QF. This includes identifying and addressing areas where Substrate's current implementation may be suboptimal for QF's specific needs and goals.\n- Interview experienced Substrate developers to gather insights into potential areas for improvement and identify pain points in the existing Substrate codebase.\n- Analyze and optimize the most frequently executed \"hot path\" code within the Substrate runtime to achieve significant efficiency gains (reduction in CPU utilization and memory consumption) and increase execution speed through algorithmic optimization, data structure optimization, and code-level optimizations.\n- Identify, analyze, and optimize performance bottlenecks in the existing Substrate code, focusing on runtime and storage performance using profiling tools.\n- Explore the possibility of using BLAKE3 as a replacement for the default hash function, and implement it if possible, aiming for an improvement in hash generation speed.\n- Identify and implement hardcoding opportunities to reduce overhead, exploring trade-offs related to performance, flexibility and maintainability.\n- Research and define methods to improve the debuggability of the system, focusing on tools and techniques that make it easier to identify, track, and resolve problems.\n- Test that all changes maintain backwards compatibility with existing QF Substrate components and interfaces.\n- Evaluate if the selected optimizations require a Substrate fork or can be implemented using a patch, and analyze the impact of choosing one of the approaches.\n- Establish a clear Substrate baseline and define specific benchmarks for each area of improvement (e.g., smart contract execution, storage access, transaction processing) to measure performance gains.\n- Develop a methodology and benchmarking scenarios for evaluating the performance of the optimized QF blockchain. This includes defining specific metrics, test cases, and data sets that accurately reflect real-world usage patterns.\n- Develop and document benchmark tests that represent realistic workloads for the QF blockchain. Particular use cases and scenarios for these benchmark tests will be explored in a separate product research phase. The benchmarks should include metrics such as:\n    - Transactions per second;\n    - Transaction latency;\n    - Block time;\n    - Block size;\n    - Finality time;\n    - CPU and memory utilization;\n    - Network bandwidth consumption;\n    - Storage I/O performance (e.g. read/write IOPS, latency).\n- Integrate benchmark tests into the CI/CD pipeline to continuously track performance improvements.\n- Investigate the impact of consensus mechanisms, network latency, and node infrastructure on performance, with a focus on selecting the most optimal microparameters for QF. This includes fine-tuning consensus parameters, network configurations, and node settings to maximize performance.\n- Use separate benchmarks for different use cases to accurately reflect real-world scenarios. This will ensure the benchmarks provide more granular insights into performance improvements in specific areas.\n- Analyze how replacing the network, storage, consensus, and RPC layers will affect the architecture of the rest of Substrate and address any imbalances found. This ensures that the optimizations are integrated seamlessly and do not introduce new architectural issues.\n\n## Acceptance Criteria\n- [ ] QF-specific Substrate optimizations deliver measurable improvements by benchmarking QF-specific workloads against baseline Substrate and documenting at least 2x performance gain in target areas.\n- [ ] A well-documented bottleneck analysis has been performed and is present in the final report, that identifies which parts of code need to be improved.\n- [ ] Substrate developer feedback on codebase pain points and optimization opportunities provides actionable insights by conducting structured interviews with at least 5 experienced Substrate developers (3+ years experience) documenting findings in detailed report.\n- [ ] A working implementation of the optimized code, or a fork of Substrate, demonstrates a performance improvement over a baseline Substrate implementation, measured using benchmark tests that represent realistic workloads.\n- [ ] Demonstrable improvements in \"hot path\" code execution speed are confirmed by measuring it before and after optimization, expressed as a reduction in CPU utilization (measured by profiling tools like perf) and reduction in memory consumption (measured by tools like perf, memory-stats or dhat-rs).\n- [ ] A report of the specific hardcoding choices and the benefits that they produce on performance is provided. This should include an explanation of the trade-offs made.\n- [ ] If the BLAKE3 hashing algorithm is implemented as the universal hashing primitive, it is done correctly and tested to avoid any unintended consequences. Benchmark results should show an improvement in hash generation speed compared to the default Substrate implementation.\n- [ ] A document is provided containing code snippets and example outputs that demonstrate improvements to debuggability through the implementation of new tooling and methods. This document should compare the debugging process using baseline Substrate to the optimized version, showing improvements based on developer feedback and measuring Time to Debug.\n- [ ] Analysis identifies which changes will break backwards compatibility and would require a Substrate fork.\n- [ ] The report contains a detailed explanation about the choice between a Substrate fork or patch, with a complete explanation of its impact to the codebase.\n- [ ] A dashboard is implemented that automatically downloads a fresh nightly build of Substrate every night, runs the defined benchmark tests, and plots a graph of the results over time. This dashboard should provide a clear visualization of performance trends and help identify any regressions introduced by new Substrate updates.\n- [ ] Architecture balance assessment validates optimization impacts by analyzing cross-layer dependencies and documenting any required adjustments.\n- [ ] A comprehensive report is provided, detailing the performance improvements achieved in each area.\n\n## Related Work\n- Substrate framework documentation: [docs.substrate.io](https://docs.substrate.io/) and [docs.polkadot.com](https://docs.polkadot.com/develop/parachains/intro-polkadot-sdk/#substrate)\n- BLAKE3 [Rust and C implementations](https://github.com/BLAKE3-team/BLAKE3), [IETF Internet-Draft](https://www.ietf.org/archive/id/draft-aumasson-blake3-00.html) and [paper](https://github.com/BLAKE3-team/BLAKE3-specs/blob/master/blake3.pdf).\n- [Polkadot Documentation: runtime-metrics-monitoring](https://docs.polkadot.com/develop/parachains/maintenance/runtime-metrics-monitoring).\n- [Jaeger tracing platform](https://www.jaegertracing.io/).\n\n## Constraints\n- The optimizations must maintain the security and decentralization of the blockchain.\n- The optimizations must be compatible with the Substrate framework, either through a patch or with a small Substrate fork.\n- The optimizations must not compromise the forkless runtime upgrade mechanism.\n- The optimizations must not break or make debugging harder.\n\n## Risks\n- Potential introduction of bugs during code optimization, making the system less stable or vulnerable to attacks.\n- Possible unforeseen performance issues due to hardcoded design decisions, especially with new Substrate updates.\n- Risk of not achieving the target x2-x3 performance improvement despite all efforts.\n- Backwards incompatibility of current QF functionalities due to changes in Substrate primitives.\n- Choosing to fork Substrate may create unforeseen issues that might prevent further updates, or that it will make further debugging and development more complex.\n- The project might have lower appeal for new developers if the code is too different from the current Substrate implementation, and they will not be able to reuse their current skills.\n\n## Development Outcomes\n- Implemented optimizations for the \"hot path\" code, with metrics on performance gains.\n- A clearly defined list of hardcoded design decisions, to reduce overhead, and they will be implemented.\n- A working BLAKE3 implementation and a benchmark comparing its usage with the default Substrate primitives, and a documentation on how it is integrated into the blockchain.\n- A clear guide on how to use the improved debugging tools.\n- Complete benchmark results for the QF blockchain, showing a x2-x3 performance gain compared to the baseline.\n- A detailed report analyzing the trade-offs and impact of using a Substrate fork vs. a patch.\n- A detailed report about performance bottlenecks that were identified and addressed during development.\n- A comprehensive set of benchmarks that measure the performance of key blockchain operations.\n- Clear and detailed reports on the performance improvements achieved in each area, with comparisons to the established Substrate baseline.",
                "url": "https://github.com/QuantumFusion-network/spec/issues/15",
                "comments": []
            },
            {
                "number": 14,
                "title": "RPC layer",
                "state": "open",
                "author": "actinfer",
                "labels": [
                    "Accepted"
                ],
                "created_at": "2024-12-15T05:57:40Z",
                "updated_at": "2025-03-06T11:15:58Z",
                "closed_at": null,
                "body": "The RPC is a foundational component of QF on making Web3 development as accessible as Web2. By leveraging Cap'n Proto for RPC implementation, QF aims to provide a high-performance, developer-friendly interface that bridges the gap between Web2 and Web3 development paradigms.\n\n### Objectives\n- Build a robust RPC layer using Cap'n Proto for efficient and type-safe communication\n- Design an intuitive Web2-like RPC interface, libraries, and documentation for seamless Web3 development\n- Achieve latency and uptime lower than leading RPC implementations through efficient serialization\n- Provide seamless integration with Polkadot's Substrate framework\n- Create an intuitive abstraction layer over blockchain complexities for the most popular use cases\n- Generate client libraries for major programming languages: JavaScript, Rust, Python\n- Implement robust error handling with Web2-style clarity\n- Create detailed documentation, including examples, API references, and real-world guides\n\n\n### Acceptance Criteria\n- [ ] Cap’n Proto implementation ensures type-safe interfaces, efficient serialization, and support for real-time data streaming\n- [ ] Basic operations respond in 100ms timeframes in 95% of requests, with 99.9% uptime validated under load tests\n- [ ] JavaScript client library is published to GitHub and npm at version 1.0.0 with installation and usage instructions\n- [ ] Developer setup time is comparable to standard Web2 frameworks, with intuitive error messages, integrated debugging tools, and comprehensive documentation with practical examples\n- [ ] Detailed, sufficiently clear, and comprehensive for developers is available. Covering all aspects, functionality, and usage, to a level that makes them approachable for new developers.\n\n### Related Work\n- Cap'n Proto serialization protocol (https://capnproto.org/).\n- Polkadot RPC API (https://polkadot.js.org/docs/api)\n- WebSocket networking protocol (https://websockets.spec.whatwg.org/)\n- The BLAKE3 Hashing Framework (https://www.ietf.org/archive/id/draft-aumasson-blake3-00.html)\n\nCompetitors:\n- Ethereum RPC API (https://ethereum.org/en/developers/docs/apis/json-rpc/)\n- Solana RPC API (https://solana.com/docs/rpc)\n\n### Constraints\n- Must maintain compatibility with Polkadot's ecosystem and existing blockchain standards\n- Need backward compatibility and clear upgrade paths\n\n### Risks\n- Difficulty in aligning with existing Polkadot ecosystem tools may hinder the deployment of the RPC layer with Cap'n Proto\n- Challenges in achieving sub-100ms latency could impact the goal of high performance and 99.9% uptime\n- Technical issues in implementing subscription support could affect reliable real-time updates\n- Issues in developing JavaScript client libraries could hinder the objective of providing comprehensive client support\n\n### Learning Outcomes\n- Documentation quality significantly impacts blockchain adoption \n- RPC API compatibility requires standardized approaches \n- WebSocket reliability critical for dApps (universal issue across providers)\n- Simplified SDKs with examples accelerate developer onboarding \n- Clear versioning and upgrade paths essential \n- Technical analysis report comparing Cap'n Proto implementation versus traditional JSON-RPC approaches in blockchain landscape\n- API specification document detailing type-safe interfaces and serialization methods \n- Load testing results report validating performance under various conditions\n\n### Decision Log\n- Decision 1: Adopt Cap'n Proto for RPC implementation to ensure type safety and performance\n- Decision 2: Implement subscription support (WebSocket) for real-time updates\n- Decision 3: Implement unified error handling system with clear messages and integrate debugging tools \n- Decision 4: Create extensive documentation with practical examples\n\n### Notes\n- Regular feedback from Web2 developers crucial for improving accessibility\n- Focus on practical examples and tutorials in technical documentation\n- Prioritize backward compatibility and clear upgrade paths\n",
                "url": "https://github.com/QuantumFusion-network/spec/issues/14",
                "comments": []
            },
            {
                "number": 13,
                "title": "Blockchain state storage research and optimisation",
                "state": "open",
                "author": "actinfer",
                "labels": [
                    "In Use"
                ],
                "created_at": "2024-12-15T05:56:18Z",
                "updated_at": "2025-02-20T11:48:28Z",
                "closed_at": null,
                "body": "QMDB is an embedded key-value store designed by [layerzero](https://github.com/LayerZero-Labs/qmdb) team specifically for storing Merkelized data in the blockchain context.\n\nThis case focuses on implementing and deploying efficient blockchain data storage with QMDB as a replacement for Substrate's current storage, and optimizing state access for full nodes. This development includes solving QMDB's limitations, improving its data access, and hardening security when used with Substrate. Our aim is to implement optimizations for QMDB to handle smart contract data, improve synchronisation, and enhance collator performance.\n\n## Objectives\n- Implement Substrate integration, specifically deploying QMDB as a replacement for existing database, specifically delivering the core features (inserting and retrieving data; generating Merkle proofs i.e. ProofStorage).\n- Solve QMDB's limitations such as the prefix search problem.\n- Develop efficient blockchain state access for full nodes, so it can be retrieved quickly when needed, keeping in mind that every byte counts, and ensure the data is easily accessible, including implementing data retrieval with prefix operations and bounds.\n- Implement persistent in-memory indexes to support prefix operations on QMDB using indexer traits.\n- Build storage system for data that is changed by our smart contracts, so full nodes can use it with new state storage such as QMDB.\n- Develop and deploy different options for synchronizing chain data between full nodes.\n- Implement methods to optimize collators by using QMDB, and integrate QMDB into Substrate for parachains.\n- Fix any security risks caused by the changes we implement for data storage, including hardening against attacks on the chain and when replacing existing Substrate implementation.\n- Implement a HashDB-like interface for accessing the blockchain state.\n- (Optional) Deploy different ways to store the blockchain state (e.g., Merkle Trees, Patricia Trees, databases), with optimizations for data size and Substrate integration.\n- Document the performance characteristics of the implemented methods, including which implementations work better with Substrate, with metrics on QMDB performance, solutions to replacing existing implementation and fixes for QMDB's prefix search and key size issues, and verification of tree-based signature requirements.\n\n## Acceptance Criteria\n- [ ] QMDB is integrated with Substrate, with complete implementation solving limitations, including performance metrics when used as a replacement for existing databases.\n- [ ] QMDB's limitations, specifically the prefix search problem and limitations on key sizes, have been solved and tested in production.\n- [ ] Methods for efficiently accessing the blockchain state for full nodes are implemented and deployed, including working bounded and prefix operations.\n- [ ] Persistent in-memory indexer are implemented and proven to support prefix operations on QMDB, with performance metrics and limitations documented.\n- [ ] Storage system for smart contract data changes is implemented and accessible to full nodes through QMDB.\n- [ ] Different options for synchronizing chain data between full nodes are implemented and deployed.\n- [ ] Collator optimization using QMDB is implemented and integrated into Substrate for parachains, with performance metrics.\n- [ ] All security risks are identified and fixed, including those related to replacing existing Substrate storage with QMDB, with hardening against new attack vectors.\n- [ ] HashDB-like interface is implemented and deployed.\n- [ ] (Optional) Different blockchain state storage implementations are deployed and compared (e.g., Merkle Trees, Patricia Trees, databases), with optimizations for data size and Substrate integration.\n- [ ] Complete technical documentation covers all implementations and optimizations, with emphasis on QMDB performance and solutions to QMDB limitations.\n\n## Related Work\n- [QMDB GitHub repo](https://github.com/LayerZero-Labs/qmdb)\n- [Substrate Storage Documentation](https://docs.substrate.io/learn/state-transitions-and-storage/)\n- [Merkle Tree Documentation](https://en.wikipedia.org/wiki/Merkle_tree)\n- [RocksDB Documentation](https://rocksdb.org/docs)\n- [`sp-trie` implementation](https://crates.io/crates/sp-trie)\n- [`sp-trie` GH](https://github.com/paritytech/trie)\n\n## Constraints\n- The state storage methods must work well with Substrate framework and be compatible with its primitives, and must be able to replace existing implementation.\n- The changes must not require a hard fork of the blockchain.\n- The state storage implementation must be able to support 0.1-0.3 second block production speed.\n\n## Risks\n- Changes to storage could cause our network to become less stable and reliable.\n- New storage methods might not be compatible with future Substrate updates.\n- A `QMDB` implementation could introduce compatibility issues with the Polkadot relay chain due to different assumptions about data storage and structure.\n- It might be difficult to make data storage faster without making it less secure or by making the new implementation unstable or not performant in the long term, or by adding overhead because of a new implementation of prefix or bound operations.\n- Implementation limitations of `QMDB `, or data loss due to its current development stage.\n- The new data storage methods with `QMDB` may have security risks that we don't know about\n- There is a high risk that problems with replacing the existing primitives can create unforeseen issues, and new interfaces might not match the needs of the Substrate runtime, and the chosen implementation may have limitations.\n- It may be difficult to achieve a performance gain over current state storage solutions, while maintaining all current functionality.\n- Optimizations may not be effective across different hardware and network configurations.\n\n## Development Outcomes\n- A working QMDB optimization that reduces data size, improves access speed, with implemented prefix and bounded operations, deployed in-memory indexes, and solutions to trade-offs when replacing sp-trie and fixing QMDB limitations.\n- Production implementation showing how QMDB works internally, including measured benefits and drawbacks for blockchain performance.\n- Complete implementation of Substrate primitives needed for custom storage solution and integration with QMDB.\n- A complete refactoring of Substrate crates using sp-trie and their replacement with QMDB integration.\n- Working implementations of chain synchronization for full nodes integrated with QMDB.\n- Deployment of security measures addressing all potential risks related to state storage, including QMDB usage and issues, as well as implementation trade-offs and their security implications.\n- Solutions to compatibility issues with the Polkadot relay chain and implemented mitigations.\n- Complete implementation of HashDB-like interface integrating alternative storage mechanism with Substrate, and optimized implementations comparing RocksDB, sp-trie and QMDB performance.\n- (Optional) Implementation of different data storage techniques for blockchain, with working examples of Merkle Trees and alternatives to traditional key-value databases, and performance metrics between different databases like RocksDB and QMDB.\n\n## Decision Log\n- Implement optimizations for state size (e.g., by using compression, sharding, state pruning, or using Merkle proofs) for full nodes, including deploying checkpointing mechanisms where old block history can be discarded after a certain point if all nodes agree that the new state is delayed until the MainNet is launched - Consultant on meeting (25 December 2024).\n\n## Notes\nThere are no objectives to achieve light nodes support, it will be implemented in another case.",
                "url": "https://github.com/QuantumFusion-network/spec/issues/13",
                "comments": []
            },
            {
                "number": 12,
                "title": "Networking",
                "state": "open",
                "author": "actinfer",
                "labels": [
                    "Accepted"
                ],
                "created_at": "2024-12-15T05:55:23Z",
                "updated_at": "2025-01-16T12:01:24Z",
                "closed_at": null,
                "body": "This case is about using the nQUIC and WebRTC protocols for the Quantum Fusion Network. The goal is to create a fast, reliable, and decentralized network layer by adapting these existing technologies to work for blockchain. The goal is to achieve high transaction acceptance speed, low latency, support large number of light nodes (10,000+ per full node), and resist censorship.\n\n## Objectives\n- Implement nQUIC for node-to-node communication in QF, based on http3/QUIC and using the NOISE protocol for key exchange, ensuring low latency and high throughput.\n- Implement WebRTC (and, potentially, deploy required signalling infrastructure) for browser-based dApps and light clients, enabling seamless and efficient peer-to-peer communication.\n- Configure and optimize both nQUIC and WebRTC for QF to achieve a target of 10 blocks per second and 10,000 transactions per second.\n- Create a networking layer that will use nQUIC for node-to-node communication and WebRTC for connections with clients in browsers, and provide access to it through common interfaces. This layer will:\n    - Automatically select the best protocol (nQUIC or WebRTC) for each connection based on the type of client.\n    - Provide a single API for sending and receiving data, regardless of the underlying protocol.\n    - Manage peer discovery and connections with other nodes, and browsers.\n- Implement and test the communication flow using nQUIC and WebRTC for standalone full nodes outside the browser.\n- Ensure that the implementation of nQUIC and WebRTC is resilient to censorship and other types of network interference.\n- Tune the network protocol to support a large number of light client connections per full node, aiming for at least 10,000 connections per full node.\n- Make a decision if the nQUIC protocol will be implemented using a Substrate fork or through a patch, and justify that decision, providing a detailed description of the impact.\n- Provide a basic UI (or telemetry support) for monitoring network connections, data transfer, and network performance of a node.\n- Create comprehensive documentation for nQUIC and WebRTC implementation and configuration in QF.\n\n## Acceptance Criteria\n- [ ] nQUIC is implemented correctly for node-to-node communication, with logs of both nodes showing successful nQUIC handshakes using the NOISE protocol.\n- [ ] The nQUIC implementation demonstrates improved performance over traditional p2p network protocols by comparing the connection times with baseline tests, using simple test scripts.\n- [ ] WebRTC is deployed for browser-based dApps and light clients by connecting a light client to a QF node using a web browser, and developer console and network traffic confirms that a connection with encryption has been established.\n- [ ] nQUIC and WebRTC are configured to provide a network that supports 10 blocks per second, and this can be verified by using simple benchmark scripts and checking logs for correct block production rate.\n- [ ] nQUIC and WebRTC are configured to provide a network that supports 10,000 transactions per second, and this can be verified by using simple benchmark scripts.\n- [ ] The networking layer establishes peer-to-peer connections by running the nodes and a dApp or light client, and a successful connection should be verified, and that data flows correctly without errors, by confirming it on the UI.\n- [ ] A transaction must be initiated through a UI of a light node and it should propagate over WebRTC (as demonstrated via browser's devtools) to other nodes, and be available on the explorer.\n- [ ] Communication flow using nQUIC and WebRTC is tested and working with standalone full nodes by running a full node outside the browser, and its logs should show that it is connecting with other nodes using WebRTC.\n- [ ] There's some evidence of our protocol choices being resilient to censhorship being presented.\n- [ ] A full node supports at least 10,000 light node connections, as shown by deploying multiple virtual machines with light clients, and verifying that a test full node can connect to all of them.\n- [ ] A decision on whether to implement nQUIC as a Substrate fork or patch is documented, and the provided document should contain the reasoning behind the chosen approach.\n- [ ] Clear and complete documentation is available for nQUIC and WebRTC implementation, setup, and usage within QF. Testing this is to review the documentation and confirm that all main steps are present, and easy to understand.\n\n## Related Work\n- Official QUIC protocol [website](https://quicwg.org/) and a [list of implementations](https://github.com/quicwg/base-drafts/wiki/Implementations).\n- [Cloudflare website](cloudflare-quic.com) for testing browsers on QUIC compatibility.\n- [WebRTC Documentation](https://webrtc.org/): Official documentation for WebRTC.\n- NOISE Protocol [official website](http://noiseprotocol.org/) and [specification](https://noiseprotocol.org/noise.pdf). The website presents open source implementations in C, C#, Go, Haskell, Java, Javascript, Python, and Rust.\n- [Noise Explorer](https://noiseexplorer.com/): Tool for exploring Noise protocol framework.\n- [nQUIC paper](https://eprint.iacr.org/2019/028.pdf).\n- [QUIC-Noise Specification](https://github.com/quic-noise-wg/quic-noise-spec).\n- [NOISE protocol integration into the Quinn QUIC library ](https://github.com/ipfs-rust/quinn-noise) in Rust for IPFS.\n- [NOISE protocol implementation](https://github.com/blckngm/noise-rust) in Rust.\n\n## Constraints\n- Browser limitations may appear and require some workarounds to get work under the memory and performance constraints.\n- WebRTC connections must be stable over mobile Internet and other poor conditions.\n- Protocols must be designed to minimize overhead and latency.\n- Protocols must be secured to avoid data leaks or attacks.\n- Must enable connections both inside and outside browsers.\n\n## Risks\n- Incorrect configuration of nQUIC and WebRTC might not provide the intended performance gains, or provide even lower performance than existing protocols, failing to reach block and transactions per second targets.\n- Integration of nQUIC and WebRTC may have unexpected compatibility issues with the rest of the QF network and other services or tools that rely on those network protocols, including issues arising from a decision to fork Substrate or not.\n- Improper implementations might expose security vulnerabilities due to improper security controls for nQUIC and WebRTC or to the lack of knowledge of the technologies.\n- Implementing and configuring both nQUIC and WebRTC may introduce development complexities and delays, especially for low level integration to the Substrate.\n- Testing all the network conditions might be hard to fully simulate, especially with a large amount of light clients, and different types of network configurations and limitations.\n- It may be hard to ensure that the network is fully resistant to censorship with changes to network configuration, or due to implementation flaws.\n- The decision to implement nQUIC as a Substrate fork or patch may have long-term implications for the maintainability, upgradeability, and future compatibility of the QF network, and an incorrect choice may limit our options.\n\n## Development Outcomes\n- A working nQUIC implementation for node-to-node communication, with NOISE protocol for handshakes, providing low-latency and high-throughput data transfer, and achieving the 10 block per second rate.\n- A secure and working WebRTC implementation for browser-based light clients and dApps that allows seamless peer-to-peer connections.\n- A working system for establishing connections between nodes and clients, using nQUIC and WebRTC accordingly, that also supports standalone full nodes.\n- Documented system-level configuration parameters to improve performance and lower latency for both protocols, including the ability to reach the 10,000 transactions per second goal.\n- Detailed documentation on how to configure, use, and troubleshoot nQUIC and WebRTC in QF, for different types of deployments.\n- A clear description on how WebRTC works for the full nodes, which are running outside the browser.\n- A configuration that allows supporting at least 10,000 concurrent light node connections per full node.\n- A short document describing how a baseline test will compare WebRTC and nQUIC node-to-node connection methods and their limitations.\n- A report that includes nQUIC benchmark result against the baseline test and simple listener code.\n- A report that includes WebRTC benchmark result and simple listener code.\n- A very first PoC implementation of nQUIC and WebRTC listener with a single RPC call is implemented, and a clear plan for other functions that need to be migrated to this new listeners.\n- A documented decision on whether to implement nQUIC as a Substrate fork or patch, with a justification of that decision.\n- A basic UI for monitoring network connections, data transfer, and network performance.",
                "url": "https://github.com/QuantumFusion-network/spec/issues/12",
                "comments": []
            },
            {
                "number": 11,
                "title": "zkTLS",
                "state": "open",
                "author": "actinfer",
                "labels": [
                    "Accepted"
                ],
                "created_at": "2024-12-15T05:53:55Z",
                "updated_at": "2025-02-13T12:09:07Z",
                "closed_at": null,
                "body": "This case focuses on implementing zkTLS (zero-knowledge Transport Layer Security) within the Quantum Fusion Network. The primary goal is to create a robust, performant, and versatile zkTLS implementation that can securely bring Web2 data on-chain.\n\n## Objectives\n\n- Implement zkTLS to securely attest Web2 data on-chain, using existing TLS encryption to generate a verifiable and tamper-proof proof about the data.\n- Enable dApp developers to use zkTLS through the QF SDK, providing a clear API and documentation on how to generate proofs and use this data in their applications, especially when using light clients.\n- Optimize the zkTLS implementation for performance, to ensure data is available on-chain and can be used to trigger blockchain events with as little latency as possible.\n- Research and evaluate different approaches to implement a system that allows various actors to retrieve data from the web using zkTLS, including the option of a decentralized oracle system, and decide how to control access and ensure valid attestation of data.\n- (Optional) Research and define a mechanism for blockchain users to act on both sides of the data provision market ( as a data requester or as a data provider), evaluating the requirements for its implementation, and the possibility of using specific smart contracts or precompiles.\n\n## Acceptance Criteria\n\n- [ ] The zkTLS implementation allows verifiable and tamper-proof attestation of specific parts of Web2 data on-chain by allowing a user to select some text from a site, send it on chain, and that can be verified by a simple on-chain smart contract, and an external dApp will show that this text actually came from the specific website on a specific date.\n- [ ] A report is provided that explores and evaluates different approaches to implement a system that allows various actors to retrieve data from the web using zkTLS, including the option of a decentralized oracle system, and a clear decision is documented on how to control access and ensure valid attestation of data.\n- [ ] dApp developers can use zkTLS through the QF SDK by generating proofs and using this data with their applications through a clear API, and with support for light client verification.\n- [ ] Verify zkTLS implementation is optimized for performance, with verifiable proofs, and data available on-chain with as little latency as possible by checking transaction times.\n- [ ] (Optional) A mechanism for blockchain users to act on both sides of the data provision market (as a data requester and as a data provider) is defined, and a report that evaluates the requirements for its implementation and the possibility of using specific smart contracts or precompiles is created.\n\n## Related Work\n\n- [zkPass whitepaper for zkTLS implementation](https://zkpass.gitbook.io/zkpass/overview/technical-whitepaper).\n- [TLS Notary (TLSN) Protocol article](https://docs.pluto.xyz/web-proofs/tls-notary).\n- [AirTag Utility for Crypto](https://www.nascent.xyz/idea/cryptos-airtag-moment).\n\n## Constraints\n\n- zkTLS must be compatible with the existing TLS infrastructure.\n- The implementation must provide a performance that is fast and reliable, with as little overhead as possible, and compatible with the QF block times.\n\n## Risks\n\n- zkTLS implementation may introduce performance bottlenecks if not implemented correctly, and may cause issues when data is delivered on-chain.\n- The zkTLS implementation may have security vulnerabilities that could expose the data to attacks if not implemented correctly:\n    - zkTLS implementation might leak private web data or session details when creating on-chain proofs, or an attacker might be able to reconstruct the original data.\n    - zkTLS implementation might not correctly verify the source or content of Web2 data, allowing tampered data to be accepted.\n    - Attackers could pretend to be a valid web server or a user, sending fake data to the chain using zkTLS.\n    - zkTLS implementation could be vulnerable to DoS attacks that might prevent Web2 data from being attested on the blockchain, or might prevent valid data from being verified.\n    - Exploiting vulnerabilities in zkTLS might have effects on QF applications, by providing incorrect data.\n- The implementation of zkTLS and its integration with on-chain logic may require significant development effort, and may take a long time to be stable.\n\n## Development Outcomes\n\n- A working zkTLS implementation that allows secure and private attestation of Web2 data on the chain, with verifiable proofs, and that can provide proofs of existence of that data at a specific time.\n- A documented system that describes how to use zkTLS to create decentralized oracles for secure and private on-chain verification of data from Web2 sources, and the impact of using or not using multiple external providers.\n- Well-documented and easy-to-use tools and libraries in the QF SDK that enable dApp developers to use zkTLS, both for full nodes and light clients.\n- A documented system that shows how to use zkTLS to create decentralized oracles for secure and private on-chain verification of data from Web2 sources.\n- Implementation of configuration options to optimize the performance of zkTLS to reduce the latency of verification of data.\n- (Optional) A mechanism for blockchain users to act on both sides of the data provision market (as a data requester or as a data provider) using smart contracts, and the contracts are fully functional.",
                "url": "https://github.com/QuantumFusion-network/spec/issues/11",
                "comments": []
            },
            {
                "number": 10,
                "title": "Verifiable offchain workers",
                "state": "open",
                "author": "actinfer",
                "labels": [
                    "Accepted"
                ],
                "created_at": "2024-12-15T05:52:41Z",
                "updated_at": "2025-02-13T12:09:22Z",
                "closed_at": null,
                "body": "This case focuses on implementing and utilizing verifiable offchain workers within the Quantum Fusion Blockchain Network. Offchain workers will handle computationally intensive tasks that are not feasible to execute directly on the blockchain. These tasks will be verified using verifiable computation techniques, ensuring the integrity of results while maintaining the blockchain's performance, and also preserving user's privacy by not disclosing input or execution details.\n\n## Objectives\n\n- Implement offchain workers to handle complex computations (e.g., machine learning, advanced cryptography, some non-real-time games) that cannot be accommodated by the 0.1-second block time of QF, by creating a way to communicate with them from the blockchain.\n- Enable developers to create and deploy new offchain workers for specific use-cases using a clearly defined API and a testing platform, with clear ways to test their code, also considering the usage of existing frameworks for formalizing and launching worker computations.\n- Implement a system for verifiable offchain computations, so that the system can provide cryptographic validation that the computations were executed correctly. \n- Study different verification methods such as zero-knowledge proofs (ZKPs), secure multi-party computation (MPC), reputation systems with staking, and optimistic verification with challenge-response, and probabilistically checkable proofs (PCPs), and exploring if they can be implemented on-chain within the 0.1-second block time.\n- Develop a system to allow developers to seamlessly combine fast on-chain transactions with compute-intensive offchain operations, providing clear documentation on how the developers should make a decision if to use offchain computation or not.\n- Implement a mechanism for blockchain users to act on both sides of the offchain computation market (i.e., as a computation requester and as a computation provider).\n- Provide tools and libraries in the QF SDK for seamless integration of verifiable offchain computations and offchain workers into dApps, for both browser-based applications and for mobile SDKs, making it easy to use all related APIs, and focusing on different verification mechanisms (e.g. ZKPs, MPC).\n- (Optional) Implement a separate offchain worker to collect latency information between nodes.\n\n## Acceptance Criteria\n\n- [ ] Offchain workers can be deployed and can perform specific computations, and they will be able to provide verifiable results to the blockchain.\n- [ ] The system ensures that computations are performed as specified and are not tampered with by validating the output using verifiable computation mechanisms, and the results of the verification are correct.\n- [ ] The use of offchain workers does not introduce any security vulnerabilities to the blockchain, and that the integration with the verification system is secure.\n- [ ] The system is easy for developers to use, allowing seamless integration of on-chain and offchain operations, with access through light clients on the browser, or through other SDKs, and that developers have a clear way to select if to use an on-chain or offchain method to compute a specific task.\n- [ ] (Optional) Offchain workers can be used to collect latency information between nodes and provide it to the network.\n\n## Related Work\n\n- Research papers [Heiss et al., 2022](https://ieeexplore.ieee.org/document/9881809) and [Domenech et al., 2024](https://ieeexplore.ieee.org/document/10634421) as examples of cryptography that can be leveraged for zero-knowledge proofs.\n- [Polkadot](https://paritytech.github.io/polkadot-sdk/master/polkadot_sdk_docs/reference_docs/frame_offchain_workers/index.html) and [Substrate] (https://docs.substrate.io/tutorials/build-application-logic/add-offchain-workers/) documentation on offchain workers.\n- [Offchain Worker Example Pallet](https://github.com/paritytech/polkadot-sdk/tree/master/substrate/frame/examples/offchain-worker).\n- [zkPass whitepaper for zkTLS](https://zkpass.gitbook.io/zkpass/overview/technical-whitepaper).\n- [Golem](https://www.golem.network/): An example of a project attempting to build a decentralized computation platform.\n- [KIRA](https://docs.kira.network/) : Example of a verifiable computation system with a more general approach than ZKPs, using a common DSL.\n- [Pepper](https://www.pepper-project.org/) academic research project on verified computation.\n- [Herodotus Project](https://herodotus.dev/verifiable-compute) that offers verifiable computations.\n- [Jolt zero-knowledge virtual machine for RISC-V](https://a16zcrypto.com/posts/article/building-jolt/).\n- [Example of an implementation of an offchain worker by Integritee Network](https://github.com/integritee-network/worker).\n- [Comprehensive view of verifiable computation](https://www.archetype.fund/media/verifiable-compute-scaling-trust-with-cryptography) from Archetype Venture Fund.\n- [Research paper on Off-chaining Models and Approaches to Off-chain Computations](https://circle.openmesh.network/wp-content/uploads/2024/05/Off-chaining-Models-and-Approaches-to-Off-chain-Computations.pdf).\n\n## Constraints\n\n- Offchain computations must be verifiable through cryptographic methods to maintain trust and security, with as little overhead as possible to ensure correct block times, and it must support different types of verifiable computation (ZKP, MPC, or other).\n- Large data storage, verifiable computations, and the verification process itself should minimize the required storage and network bandwidth.\n- The system must provide fast access to all the functionality, such as offchain work, data availability and verifiable computation, all within the expected block times.\n\n## Risks\n\n- Potential security vulnerabilities in the implementation of verifiable computation systems and in the transport of data through offchain channels, and that malicious offchain workers may attempt to collect user data if proper protections are not put into place.\n- Difficulties in integrating offchain computations with on-chain transactions seamlessly and without adding performance penalties, and that developers may have a hard time selecting the best way to integrate the system with the rest of their projects, or may need to handle very low-level implementation details.\n- Performance issues with offchain storage when handling extremely large data files, which might impact the execution of the offchain processes and overall scalability.\n- The complexity of developing, testing, and maintaining offchain worker systems may lead to delays, and new issues and testing offchain functionality may be difficult due to its nature.\n- The offchain worker system may have issues related to scalability, requiring too many resources to operate and failing to provide enough verifications.\n- The privacy of data used on the offchain worker system may be compromised by malicious actors or by developers making mistakes, and it might expose sensitive information.\n\n## Development Outcomes\n\n- A working offchain worker implementation that can be used through the PolkaVM system.\n- A clearly defined API for developers to create and deploy new offchain workers, and test the core logic of these workers using a dedicated testing platform, and the option to use an existing framework for offchain computations.\n- A working verifiable computation mechanism that can validate offchain computations (like ZKPs, MPC or other methods), and have a clear path on when they should be used, and which also implements a process for on-chain verification.\n- Implementation of a clear API for dApps to use the offchain worker capabilities, with examples of both usage from on-chain transactions and from offchain light clients, and using the SDK from web browsers and native mobile apps.\n- Detailed documentation that clarifies how to implement and use both offchain workers and verifiable computation techniques, especially when dealing with the SDK for web browsers and native applications, and all underlying infrastructure, including documentation on testing, optimization and security implications.\n- A description on how developers should make a decision if to use on-chain or offchain computation for any specific use-case.\n- Documentation with minimal hardware requirements for running an offchain worker, and a description of the deployment process.\n- (Optional) An implementation that uses offchain workers to collect latency information between nodes, and this is being displayed on the portal, and that this system does not impose a large overhead to the system.",
                "url": "https://github.com/QuantumFusion-network/spec/issues/10",
                "comments": []
            },
            {
                "number": 9,
                "title": "Smart contract environment for TestNet",
                "state": "open",
                "author": "actinfer",
                "labels": [
                    "In Use"
                ],
                "created_at": "2024-12-15T05:50:07Z",
                "updated_at": "2025-05-29T11:02:58Z",
                "closed_at": null,
                "body": "This case focuses on developing the minimum viable product (MVP) for the PolkaVM smart contract platform within the QF project. It addresses the need for a functional and efficient smart contract execution environment with core capabilities, enabling basic smart contract deployment and interaction. This MVP is crucial because it allows developers to start building and testing dApps on QF, providing valuable feedback for future development and demonstrating the platform's core functionality.\n\n## Objectives\n\n- Integrate PolkaVM to QF blockchain.\n- Implement a fee system where users can deploy contracts by paying a fee in QF tokens.\n- Implement a permissionless system for assigning and managing unique addresses for each deployed smart contract, ensuring public access to contract interaction (similar to ABI in Ethereum) and the ability to view contract details through this address.\n- Develop a Rust library which contains a domain-specific language for writing smart contracts for PolkaVM, `qf-polkavm-sdk`.\n- Develop basic gas metering for contract execution.\n- Build public API for contract interaction with user-defined gas parameter value.\n- Develop a framework for smart contract developers with essential functions including:\n  - Basic `ink!` functions such as:\n    - `caller()`: Returns the address of the caller of the executed contract.\n    - `account_id()`: Returns the account ID of the executed contract.\n    - `balance()`: Returns the balance of the executed contract.\n    - `block_number()`: Returns the current block number.\n    - `transfer(…)`: Transfers value from the contract to the destination account ID.\n    - `balance_of(...)`: Return the balance of the given account.\n- Implement host functions for working with storage and provide API for the `qf-polkavm-sdk` to use them.\n- Create documentation for `qf-polkavm-sdk` covering smart contract development in Rust, contract compilation and deployment process, address retrieval, contract interaction methods, API usage with examples.\n- Create a set of smart contracts examples: get smart contract function caller, get contract account, balance transfer, getting given account balance, get contract balance, get block number.\n\n## Acceptance Criteria\n\n- [ ] User successfully deploys smart contract by paying QF token fee and verifying deployment success and fee deduction.\n\n**Use cases style:** As a Smart Contract Developer, I can deploy a smart contract by paying a QF token fee, so that the contract functions become callable and the deployment transaction fee is deducted from my account balance.\n\n> - Clone & compile PolkaVM SmartContract or use already compiled from [examples](https://github.com/QuantumFusion-network/qf-polkavm-sdk).\n> - Create an account and obtain tokens via the [faucet](https://dev.qfnetwork.xyz/faucet.html) or have already registered accounts with some tokens.\n> - Access [Polkadot/Substrate Portal](https://dev.qfnetwork.xyz/).\n> - Click the \"Developer\" dropdown menu in the top navigation bar and select \"Extrinsics\".\n> - In the \"submit the following extrinsic\" field, select qfPolkaVM and then upload(programBlob).\n> - Enable the \"file upload\" toggle and upload the precompiled demo smart contract.\n> - Click \"Submit Transaction\" and sign the transaction.\n> - Verify the transaction success and fee deduction from the account.\n> - To retrieve the contract address click the \"Developer\" dropdown menu in the top navigation bar and select \"Chain state\".\n> - In the \"selected state query\" field, select qfPolkaVM and then codeAddress.\n> - Select your account from the dropdown list (the account that deployed the contract).\n> - Click the \"+\" button to execute the query and view the returned contract address.\n\n- [ ] PolkaVM is integrated to QF blockchain and it is verifiable through Polkadot/Substrate Portal.\n\n**Use cases style:** As a dApp User, I can verify that PolkaVM is properly integrated with the QF blockchain, so that I can confidently execute operations on smart contracts through the standard interface.\n\n> - Open [Polkadot/Substrate Portal](https://dev.qfnetwork.xyz/).\n> - Click the \"Developer\" dropdown menu and select \"Extrinsics\".\n> - In the \"submit the following extrinsic\" field, select qfPolkaVM and then execute(contractAddress, to, value, op, gas).\n> - Fill in these parameters:\n> - contractAddress: The address obtained from the previous step (Developer > Chain state > qfPolkaVM > codeAddress),\n> - to: Target account address for transfer operations or balance queries (can use your own address for testing),\n> - value: For token transfers, enter the amount (e.g., 0 for non-transfer operations),\n> - op: Operation code as u8 (0=transfer, 1=get current contract balance, 2=get address balance, 3=get block number, 4=inf loop test),\n> - gas: Enter a minimum value of 20 as u32.\n> - Click \"Submit Transaction\" and sign with your account.\n> - Verify the transaction executes successfully in the block explorer or events tab.\n\n- [ ] Contract addressing system generates unique addresses for each deployed contract, and allows any account (not just the deployer) to execute the contract by using its address. Verifiable by deploying a test contract from one account, sharing its address, and successfully calling its functions from at least two different accounts.\n\n**Use cases style:** As a Smart Contract Developer, I can verify that the contract addressing system generates unique addresses and allows permissionless interaction, so that I can be confident in the security and openness of the platform.\n> \n> - Deploy identical smart contracts from two different accounts by following the deployment steps above.\n> - Verify they receive different addresses by:\n> - Click the \"Developer\" dropdown menu and select \"Chain state\",\n> - In the \"selected state query\" field, select qfPolkaVM and then codeAddress,\n> - For each account that deployed a contract, select that account in the dropdown,\n> - Click the \"+\" button to execute each query and compare the returned addresses to confirm they're different.\n> - Test permissionless access by:\n> - Switch to a first account,\n> - Navigate to Developer > Extrinsics > qfPolkaVM > upload,\n> - Use the contract address from a contract deployed by second account,\n> - Set appropriate parameters (e.g., op=1 to check contract balance),\n> - Submit the transaction and verify it succeeds.\n> \n\n- [ ] A Rust library developed with a domain-specific language for writing smart contracts for PolkaVM (`qf-polkavm-sdk`), source code is available in a GitHub repository. Functions available for the user are: `caller()`, `account_id()`, `balance()`, `block_number()`, `transfer()`, `balance_of()`.\n- [ ] A set of smart contract examples (get transaction caller, get contract account, balance transfer, getting given account balance, get contract balance, get block number) are available in a GitHub repository.\n>\n> - Demonstrating generated Rust doc.\n> - Open GitHub repo with examples, each example should be well documented via comments and README file.\n> - Open terminal and `copy paste` commands from the README file.\n> - Run command and see that compilation of Smart Contract done well.\n>\n\n- [ ] Gas metering tracks resource consumption during smart contract execution and it is verifiable through Polkadot/Substrate Portal.\n>\n> - Run a compiled & deployed smart contract from the previous point.\n> - The result of smart contract execution should posted event in [Polkadot/Substrate Portal](https://dev.qfnetwork.xyz/)  with information about GAS spend.\n>\n\n- [ ] Smart contract execution is interruptable. There is a configurable max gas limit and execution aborts if it reaches max gas limit.\n>\n> - Run a smart contract from & deployed the previous point via [Portal](http://portal.qfnetwork.xyz/).\n> - Run the same smart contract with GAS limit = 2.\n> - Get error `Out of Gas`.\n> - Run a smart contract with forever loop with GAS limit = 10000.\n> - Get error `Out of Gas`\n> - Using sudo change max limit from 10000 to 20000.\n> - Ensure by get() from the storage (using [Portal](http://portal.qfnetwork.xyz/), that changes are not saved into the store.\n\n- [ ] Smart contract deployment fails properly when insufficient fee is provided by attempting deployment with low balance, an attempt returns an error and it is verifiable through Polkadot/Substrate Portal.\n>\n> - Create an account (or have already created) with not enough token for smart contract deployment.\n> - Try to deploy a new smart contract using [Portal](http://portal.qfnetwork.xyz/).\n> - Get error that not enough tokens.\n> - Check using [Portal](http://portal.qfnetwork.xyz/) that balance of created account.\n>\n\n- [ ] Documentation for `qf-polkavm-sdk` is created and availabile in a GitHub repository.\n>\n> - Documentation structure:\n> - Signatures & descriptions for all Smart Contract functions\n> - Link to examples stored into the repo\n> - Manual about how to compile and how to deploy\n> - How to interact with contracts - whole path\n>\n- [ ] Implement host functions for working with storage and provide API for the `qf-polkavm-sdk` to use them.\n> - Available function meet with requirements: https://github.com/QuantumFusion-network/spec/pull/276\n\n## Related Work\n\n- [Polkadot.js Apps](https://polkadot.js.org/apps/) and [QF Block explorer](https://dev.qfnetwork.xyz/#/explorer) interface.\n- [Substrate Contract Development](https://docs.polkadot.com/develop/smart-contracts/).\n- PolkaVM [GitHub repository](https://github.com/paritytech/polkavm).\n- [Transactions fees and gas metering implementation in Polkadot SDK](https://docs.polkadot.com/polkadot-protocol/basics/blocks-transactions-fees/fees/).\n- [QF implementation of PolkaVM Blob Hashing and Addressing](https://github.com/QuantumFusion-network/spec/blob/main/docs/PolkaVM/blob_hashing_addressing.md).\n- [Ethereum ABI Specification](https://docs.soliditylang.org/en/develop/abi-spec.html).\n- [Create Polkadot dApp GitHub repo by Paritytech](https://github.com/paritytech/create-polkadot-dapp).\n- [ink! Documentation](https://use.ink/6.x/) - Reference for basic smart contract examples.\n\n## Constraints\n\n- Integration must maintain backward compatibility with existing QF runtimes.\n- Contract state access must be isolated between contracts.\n- All public interfaces must be accessible through the Polkadot/Substrate Portal.\n- Documentation must be understandable by non-Substrate developers.\n\n## Risks\n\n- Gas metering implementation might not accurately track resource consumption.\n  - Mitigation: Start with simple weight calculations and focus on reliable operation rather than precise measurement.\n- Performance issues when multiple contracts are deployed and actively used.\n  - Mitigation: Begin with modest performance goals and prioritize stability over optimal performance for MVP.\n- Public API might expose unintended contract interaction patterns.\n  - Mitigation: Implement input validation for all API endpoints, create a whitelist of allowed interaction patterns, and conduct security review of the API surface before public release.\n- Fee calculation could be inappropriate for different contract sizes/complexity.\n  - Mitigation: Start with a simple fee structure and gather data for future refinement.\n- Smart contract developer framework might not include all functions developers expect.\n  - Mitigation: Clearly document supported functions and limitations, focusing on the minimal viable set.\n\n## Development Outcomes\n\n- Working PolkaVM integration in QF runtime with tested smart contract execution.\n- Implemented fee-based contract deployment mechanism with public access.\n- Basic gas metering implementation.\n- Public contract interaction API with gas parameters.\n- Smart contract developer framework with essential functions (accounts, transfers, balances, caller address, block number).\n- Documentation covering:\n  - Basic contract development using the provided framework;\n  - Compilation workflow guide;\n  - Step-by-step deployment guide through  the Polkadot/Substrate Portal;\n  - API usage examples.\n- Test suite for contract deployment and execution.\n\n## Decision Log\n\n## Notes\n\n- Security is implemented through basic measures (gas, state isolation).\n- Future enhancements, including EVM compatibility, formal verification, advanced storage systems, contract-to-contract calls, and performance optimization will be addressed in the [PolkaVM smart contract platform advanced development](https://github.com/QuantumFusion-network/spec/issues/145) case.",
                "url": "https://github.com/QuantumFusion-network/spec/issues/9",
                "comments": [
                    {
                        "author": "dedok",
                        "created_at": "2025-01-15T07:13:29Z",
                        "body": "Experiment re-assigned to @ealataur. He will be responsible for further improvements."
                    },
                    {
                        "author": "dedok",
                        "created_at": "2025-03-12T11:29:35Z",
                        "body": "Reviewed, made consistent fixes"
                    }
                ]
            },
            {
                "number": 1,
                "title": "SPIN consensus documentation",
                "state": "open",
                "author": "kirushik",
                "labels": [
                    "In Use"
                ],
                "created_at": "2024-12-14T11:33:22Z",
                "updated_at": "2025-03-06T11:14:52Z",
                "closed_at": null,
                "body": "This case focuses on establishing the theoretical foundations for a secure and robust hybrid consensus mechanism for the SPIN network. A critical challenge - potential consensus splits between SPIN's validators and the Polkadot relay chain. This requires a novel approach that leverages the security of the Polkadot relay chain while preserving SPIN's need for fast block production. This case involves defining a mechanism that will guarantee system security by preventing double-spending, ensure that the system continues to make progress after the relay chain interactions, will define what data will be transmitted to the relay chain, explore a method to reliably promote finalized blocks, create a method to elect a block producer, define how to react if a block producer acts maliciously, and define a recovery mechanism in case of missed deadlines. The successful outcome of this case will provide a solid theoretical foundation for the subsequent development and testing of the SPIN consensus mechanism. This is a critical step, and all later work will  rely heavily on it.\n\n## Objectives\n\n-  Design a hybrid consensus mechanism for SPIN that resolves potential consensus splits between its own validators and the Polkadot relay chain. This mechanism should ensure the system's security and continuous progress.\n- Define the messages and state required to be sent to the Polkadot relay chain for validation and fault detection purposes, and define the mechanism for how blocks finalized by SPIN are promoted to Polkadot.\n-  Design a reliable block producer election mechanism, and create a mechanism to prevent and react to censorship by the block producer.\n-  Explore how to implement a self-healing mechanism to manage missed deadlines for block production.\n-  Determine the optimal type of BFT consensus for the SPIN layer, and if it is necessary to implement tree-based signature verification. Explore using Aptos BFT as an alternative consensus mechanism and the benefit of using dynamic quorums for better responsiveness and scalability.\n-  Design a mechanism for state storage and retrieval to ensure data is available to the relay chain for validation.\n-  Document the proposed design, including a detailed explanation of message flows, state management, and mechanisms for detection and recovery from consensus splits.\n\n## Acceptance Criteria\n-  [x] Conduct a literature review on SotA hybrid consensus designs.\n-  [x] Create a high-level principled view design of SPIN to be included in the Quantum Fusion Network lightpaper.\n-  [x] Collect expert opinions and feedback on SPIN from independent third-party consensus engine designers.\n-  [ ] A detailed theoretical design for a hybrid consensus mechanism is created, addressing potential splits between SPIN and Polkadot. This should describe the process that guarantees system security by preventing double-spending, ensure that the system continues to make progress after the relay chain interactions, and include a description of message flows and state transition logic.\n-  [ ] The mechanism for transferring information to and from the relay chain is well documented. This includes a list of all messages, data elements and how they are created and the complete process on how blocks from SPIN get promoted to Polkadot.\n-  [ ] The method for block producer election, and the challenge mechanism for preventing censorship, are both clearly defined, along with the specific circumstances which will trigger either one of them.\n-  [ ] A complete recovery process has been designed to address what happens if a block producer fails to produce a block on time.\n-  [ ] A comparison between GRANDPA and Aptos BFT is documented, outlining the pros and cons of each.\n-  [ ] The mechanisms to detect that SPIN has stalled are explained, including a method of recovery and what the slow mode fallback is.\n-  [ ] The data elements and processes for communication with the relay chain are defined, including: state hash, block hash, signatures, and runtime upgrades.\n-  [ ] A state storage and retrieval mechanism that ensures all necessary data is available to the relay chain is proposed, with options for where to store the data.\n-  [ ] A list of all parameters that need to be tuned during implementation, is provided, including timeouts, epoch lengths, heartbeats, and yell challenges.\n-  [ ] A comprehensive published paper outlines all design choices and potential challenges with the proposed approach.\n\n## Related Work\n\n*  [QF Network Litepaper](https://qfnetwork.xyz/litepaper).\n*  [Heimdall Documentation](https://docs.polygon.technology/pos/architecture/heimdall/introduction/).\n*  [Thunderella Paper](https://eprint.iacr.org/2017/913.pdf).\n*  [Polkadot XCM Documentation](https://paritytech.github.io/xcm-docs/).\n*  [Polkadot Parachain Documentation](https://wiki.polkadot.network/docs/learn-parachains).\n*  [From Byzantine Replication to Blockchain: Consensus is only the Beginning](https://arxiv.org/pdf/2004.14527).\n\n## Constraints\n\n*  The design must minimize the amount of data sent to the Polkadot relay chain to ensure scalability and efficiency.\n*  The design must consider the asynchronous nature of communication between the SPIN network and the relay chain.\n*  The mechanism must be implementable within Substrate's parachain framework.\n*  The mechanism must be able to handle different types of hardware and software environments.\n\n## Risks\n\n*  The hybrid consensus mechanism might introduce vulnerabilities related to message handling, state synchronization, and block finality.\n*  The slow mode fallback mechanism might not be enough for critical use-cases. If there is a persistent inability to re-elect a quorum, QF would effectively be down, despite using the relay chain to process transactions.\n*  The design may introduce new security risks, for example, around the relay chain and “yell” messages, or in the election process.\n*  The design may not be compatible with future changes to the Polkadot relay chain and will need continuous updates.\n*  Implementing the landing gear might be complex due to assumptions made by the Substrate framework.\n\n## Learning Outcomes\n\n*  Understand the challenges of hybrid consensus design, especially when interacting with a complex system like Polkadot.\n*  A clear explanation of the proposed hybrid consensus mechanism and the rationale for choosing a specific approach over other options.\n*  A detailed understanding of message flows and state synchronization between SPIN and the Polkadot relay chain.\n*  Identification of potential problems in the proposed design and possible approaches for mitigations.\n\n## Decision Log\n[Record key decisions made during the case]\n\n## Notes\nDescription based on SPIN meet 2024 November 25.",
                "url": "https://github.com/QuantumFusion-network/spec/issues/1",
                "comments": []
            }
        ]
    },
    {
        "name": "apps",
        "url": "https://github.com/QuantumFusion-network/apps",
        "issues": [
            {
                "number": 31,
                "title": "WebSocket connection to 'ws://127.0.0.1:3000/ws' failed",
                "state": "open",
                "author": "khssnv",
                "labels": [
                    "help wanted"
                ],
                "created_at": "2025-06-04T14:24:11Z",
                "updated_at": "2025-06-04T14:24:11Z",
                "closed_at": null,
                "body": "The page at https://portal.qfnetwork.xyz/ tries to access local port, which is suspicious and shouldn't happen. Why is this happening and can we stop it?",
                "url": "https://github.com/QuantumFusion-network/apps/issues/31",
                "comments": []
            },
            {
                "number": 30,
                "title": "Disable irrelevant pages",
                "state": "open",
                "author": "khssnv",
                "labels": [
                    "good first issue",
                    "help wanted"
                ],
                "created_at": "2025-06-04T14:21:38Z",
                "updated_at": "2025-06-04T14:21:38Z",
                "closed_at": null,
                "body": "Polkadot/Substrate Portal has of a set of pages accessible through the navigation bar. Upstream project deployment at https://polkadot.js.org/apps/ provides a common set of pages and has a list of all networks built with Substrate SDK (accessible through the side bar) for giving the ecosystem a single entry point to all its networks RPC endpoints. At the same time first-party deployments of the Polkadot/Substrate Portal may disable some or add custom pages, e.g., see https://apps.crust.network/. In case of QF Network we currently have redundant pages such as `Network -> Event calendar` and `Developer -> Files (IPFS)` which don't work and may confuse users. We should disable them.",
                "url": "https://github.com/QuantumFusion-network/apps/issues/30",
                "comments": []
            }
        ]
    },
    {
        "name": "qf-polkavm-sdk",
        "url": "https://github.com/QuantumFusion-network/qf-polkavm-sdk",
        "issues": [
            {
                "number": 17,
                "title": "Insufficient description of prerequisites and toolchain installation guide",
                "state": "open",
                "author": "khssnv",
                "labels": [],
                "created_at": "2025-05-19T07:44:03Z",
                "updated_at": "2025-05-19T07:46:29Z",
                "closed_at": null,
                "body": "Would be good to write list of prerequisites what tools users would need, like rustup, and what components they would need.",
                "url": "https://github.com/QuantumFusion-network/qf-polkavm-sdk/issues/17",
                "comments": [
                    {
                        "author": "khssnv",
                        "created_at": "2025-05-19T07:45:56Z",
                        "body": "Also let's mention directories where the user is expected execute guide commands?"
                    }
                ]
            }
        ]
    }
]